{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration and Visualization of Cosmic Strings in CMB\n",
    "This notebook explores the Cosmic Microwave Background (CMB) data to identify potential cosmic string candidates."
   ],
   "id": "623af29cb778d5b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initial exploration of the CMB data using Healpy and Matplotlib.",
   "id": "398cb6ea7138181e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "# pip freeze > requirements.txt\n",
    "!pip install -r requirements.txt"
   ],
   "id": "69c5c13035d2a730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First exploring the COM_CMB_IQU-smica_2048_R3.00_full.fits file to understand its structure and contents.",
   "id": "8505d0be0a0656f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.constants.codata2014 import alpha\n",
    "from scipy.constants import h, c, k\n",
    "import healpy as hp\n",
    "\n",
    "# Planck function (spectral radiance)\n",
    "def planck(f, T):\n",
    "    x = h * f / (k * T)\n",
    "    return (2 * h * f**3 / c**2) / np.expm1(x)\n",
    "\n",
    "\n",
    "# Frequency range (Hz)\n",
    "frequencies = np.linspace(1e10, 1e12, 1000)  # 10 GHz to 1000 GHz\n",
    "temperature = 2.725  # CMB temperature in Kelvin\n",
    "\n",
    "# Compute spectral radiance\n",
    "radiance = planck(frequencies, temperature)\n",
    "\n",
    "# Plot spectrum\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, radiance, color='darkblue', label=f'T = {temperature} K')\n",
    "plt.title('CMB Blackbody Spectrum at T = 2.725 K')\n",
    "plt.xlabel('Frequency (GHz)')\n",
    "plt.ylabel('Spectral Radiance B(ν) [W·sr⁻¹·m⁻²·Hz⁻¹]')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4c33cfad7c5af566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import healpy as hp\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "\n",
    "# --- Load the CMB temperature map ---\n",
    "cmb_map = hp.read_map('data/COM_CMB_IQU-smica_2048_R3.00_full.fits', field=0)\n",
    "\n",
    "# --- Optional: Apply Galactic/common mask ---\n",
    "load_mask = False\n",
    "\n",
    "if load_mask:\n",
    "    cmb_mask = hp.read_map('data/COM_Mask_CMB-common-Mask-Int_2048_R3.00.fits', field=0)\n",
    "\n",
    "    # Sanity check: Ensure matching resolution\n",
    "    assert hp.get_nside(cmb_map) == hp.get_nside(cmb_mask), \"NSIDE mismatch between map and mask!\"\n",
    "\n",
    "    # Apply the mask (set bad pixels to 0)\n",
    "    cmb_map = cmb_map * cmb_mask\n",
    "\n",
    "# --- Convert to 2D Mollweide projection ---\n",
    "img = hp.mollview(\n",
    "    cmb_map,\n",
    "    return_projected_map=True,\n",
    "    nest=False,\n",
    "    title='CMB Temperature Map',\n",
    "    cmap='inferno',\n",
    "    xsize=2000\n",
    ")\n",
    "\n",
    "# --- Optional enhancement: Histogram equalization for better contrast ---\n",
    "img_eq = exposure.equalize_hist(img)"
   ],
   "id": "f4d3f330ff83f3fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Apply Sobel edge detection to reveal structure ---\n",
    "edges = ndimage.sobel(img_eq)\n",
    "\n",
    "# --- Display the edges ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title(\"Edge Detection on Equalized CMB Map\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2dfa72a8288c9645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Load or define your image (already done)\n",
    "# img = ...\n",
    "\n",
    "# --- Step 2: Force-clean the CMB image ---\n",
    "# Replace NaNs, -inf, +inf with zeros (or median)\n",
    "img_cleaned = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# --- Step 3: Edge detection ---\n",
    "edges = ndimage.sobel(img_cleaned)\n",
    "\n",
    "# --- Step 4: Print CLEANED ranges ---\n",
    "print(\"Cleaned CMB image range:\", np.min(img_cleaned), \"to\", np.max(img_cleaned))\n",
    "print(\"Edge image range:\", np.min(edges), \"to\", np.max(edges))\n",
    "\n",
    "# --- Step 5: Normalize for plotting ---\n",
    "def normalize(arr):\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return (arr - np.min(arr)) / (np.ptp(arr) + 1e-8)\n",
    "\n",
    "img_norm = normalize(img_cleaned)\n",
    "edges_norm = normalize(edges)\n",
    "\n",
    "# Optional: Threshold edges for clarity\n",
    "edges_thresh = np.where(edges_norm > 0.3, 1.0, 0.0)\n",
    "\n",
    "# --- Step 6: Plot overlay ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(img_norm, cmap='coolwarm',alpha=0.9, origin='lower', aspect='auto')\n",
    "plt.imshow(edges_thresh, cmap='YlOrBr', alpha=0.9, origin='lower', aspect='auto')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title(\"CMB with Edge Overlay (cleaned)\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cmb_edge_overlay_cleaned.png\", dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "4b455d9a45c7e271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Nonlinear exaggeration\n",
    "edges_exaggerated = edges**0.8  # or try 2.0\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(edges_exaggerated, cmap='turbo')\n",
    "plt.title('Possible Early Universal Topology Signatures (CMB Edge Detection)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "id": "2b9e6d04a881e700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sharp_edges = edges + 0.7 * edges  # amplify edge intensity\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(sharp_edges, cmap='inferno')\n",
    "plt.title('Possible Cosmic String Candidates (Edges in CMB)')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "9fea3233b6b0440d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Alter this code block\n",
    "# Apply edge detection (e.g., Sobel)edges = ndimage.sobel(img)\n",
    "# Plot the edges detected in the CMB map\n",
    "# Apply Sobel in x and y directions\n",
    "dx = ndimage.sobel(img, axis=0) ** 0.5\n",
    "dy = ndimage.sobel(img, axis=1) ** 0.5\n",
    "\n",
    "# Gradient magnitude\n",
    "edges = np.hypot(dx, dy)  # Same as sqrt(dx**2 + dy**2)\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(edges, cmap='inferno')\n",
    "plt.title('Possible Cosmic String Candidates (Edges in CMB)')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "ef325655c7cb6d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frequencies = np.linspace(10e9, 1000e9, 1000)  # 10 GHz to 1000 GHz\n",
    "\n",
    "T_mean = 2.725\n",
    "delta_T = 100e-6  # 100 µK typical fluctuation\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean), label=\"T = 2.725 K\", color='blue')\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean + delta_T), '--', label=\"+100 µK\", color='green', alpha=0.8)\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean - delta_T), '--', label=\"-100 µK\", color='red', alpha=0.8)\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Spectral Radiance (W·sr⁻¹·m⁻²·Hz⁻¹)\")\n",
    "plt.title(\"CMB Spectrum with Fluctuation Bounds (±100 µK)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the CMB spectrum with +100 µK fluctuations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean), label=\"T = 2.725 K\", color='blue')\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean + delta_T), '--', label=\"+100 µK\", color='green', alpha=0.8)\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Spectral Radiance (W·sr⁻¹·m⁻²·Hz⁻¹)\")\n",
    "plt.title(\"CMB Spectrum with Fluctuation Bounds (±100 µK)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the CMB spectrum with -100 µK fluctuations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean), label=\"T = 2.725 K\", color='blue')\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean - delta_T), '--', label=\"-100 µK\", color='red', alpha=0.8)\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Spectral Radiance (W·sr⁻¹·m⁻²·Hz⁻¹)\")\n",
    "plt.title(\"CMB Spectrum with Fluctuation Bounds (±100 µK)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "15eadc28ec4474e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cl = hp.anafast(cmb_map)\n",
    "plt.plot(cl[:200])\n",
    "plt.title(\"Low-ℓ Multipoles – Texture Signature Region\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "de96fe4776a656fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# --- Compute power spectrum from CMB map ---\n",
    "cl = hp.anafast(cmb_map)         # Power spectrum\n",
    "ell = np.arange(len(cl))         # Multipole indices\n",
    "\n",
    "# --- Begin Plot ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot measured CMB power spectrum (excluding monopole)\n",
    "plt.loglog(ell[1:1500], cl[1:1500], label='CMB Power Spectrum', color='blue')\n",
    "\n",
    "# Plot theoretical scale-invariant reference curve\n",
    "ref_value = np.mean(cl[5:15])\n",
    "theory_curve = ref_value * (ell[5] * (ell[5] + 1)) / (ell[1:1500] * (ell[1:1500] + 1))\n",
    "plt.loglog(ell[1:1500], theory_curve, 'r--', label='Scale-invariant (∝ 1/ℓ(ℓ+1))')\n",
    "\n",
    "# --- Highlight Regions ---\n",
    "plt.axvspan(2, 30, color='gray', alpha=0.15, label='Sachs-Wolfe Region')\n",
    "plt.axvspan(50, 200, color='orange', alpha=0.08, label='Transition Region')\n",
    "plt.axvspan(200, 1500, color='green', alpha=0.08, label='Acoustic Peaks Region')\n",
    "\n",
    "# --- Annotations (repositioned to avoid overlap) ---\n",
    "plt.text(7, 2e-10, 'Sachs-Wolfe\\nSuper-horizon modes', fontsize=10, color='black')\n",
    "plt.text(60, 5e-20, 'Transition:\\nEarly Oscillations &\\n Projection Effects', fontsize=10, color='black')\n",
    "plt.text(300, 5e-12, 'Acoustic Peaks:\\nPhoton-Baryon Oscillations', fontsize=10, color='black')\n",
    "\n",
    "# 1st Peak marker\n",
    "plt.axvline(220, color='black', linestyle='--', alpha=0.5)\n",
    "plt.text(235, 1e-10, '1st Peak\\n(~ℓ=220)', fontsize=9, color='black')\n",
    "\n",
    "# --- Labels, Legend, Grid ---\n",
    "plt.title(\"CMB Angular Power Spectrum (Log Scale)\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.2)\n",
    "\n",
    "# --- Format axes using human-readable numbers ---\n",
    "def human_format(x, pos):\n",
    "    if x >= 1:\n",
    "        return f\"{int(x)}\"\n",
    "    else:\n",
    "        return f\"{x:.1g}\"\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(human_format))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(human_format))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "df030401fd974066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the power spectrum (ℓ = 0 to 199)\n",
    "ell = np.arange(len(cl))  # Define ell as the array of multipole indices\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(ell[:250], cl[:250], label='CMB Power')\n",
    "\n",
    "# Overlay topological defect regions\n",
    "plt.axvspan(2, 10, color='blue', alpha=0.3, label='Textures (ℓ ≈ 2–10)')\n",
    "plt.axvspan(2, 5, color='red', alpha=0.3, label='Domain Walls (ℓ ≈ 2–5)')\n",
    "plt.axvspan(50, 200, color='green', alpha=0.2, label='Cosmic Strings (ℓ ≈ 50–200)')\n",
    "plt.axvspan(2, 3, color='orange', alpha=0.4, label='Monopoles (ℓ ≈ 2–3)')\n",
    "\n",
    "# Labels and grid\n",
    "plt.title(\"CMB Multipole Spectrum with Topological Defect Regions\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "395f01dcb190477c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the power spectrum (ℓ = 0 to 199)\n",
    "ell = np.arange(len(cl))  # Define ell as the array of multipole indices\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(ell[:50], cl[:50], label='CMB Power')\n",
    "\n",
    "# Overlay topological defect regions\n",
    "plt.axvspan(2, 10, color='blue', alpha=0.3, label='Textures (ℓ ≈ 2–10)')\n",
    "plt.axvspan(2, 5, color='red', alpha=0.3, label='Domain Walls (ℓ ≈ 2–5)')\n",
    "plt.axvspan(2, 3, color='orange', alpha=0.4, label='Monopoles (ℓ ≈ 2–3)')\n",
    "\n",
    "# Labels and grid\n",
    "plt.title(\"CMB Multipole Spectrum with Topological Defect Regions\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ea844d697ff10b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use same `cl` from previous block\n",
    "ell = np.arange(1, 200)  # Skip ℓ = 0 to avoid divide-by-zero\n",
    "theta_deg = 180 / ell    # Angular scale in degrees\n",
    "\n",
    "# Trim cl accordingly\n",
    "cl_trim = cl[1:200]\n",
    "\n",
    "# Plot C_ell vs angular scale θ\n",
    "plt.plot(theta_deg, cl_trim)\n",
    "plt.title(\"CMB Power Spectrum vs Angular Scale\")\n",
    "plt.xlabel(\"Angular Scale θ [degrees]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.gca().invert_xaxis()  # Larger angular scales on the left\n",
    "plt.show()"
   ],
   "id": "7638a875a151a36f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ℓ = 1 to 199 (skip ℓ = 0 to avoid division by zero)\n",
    "theta_deg = 180 / ell\n",
    "cl_trim = cl[1:200]\n",
    "\n",
    "# Plot power vs angular scale\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(theta_deg, cl_trim, label='CMB Power')\n",
    "\n",
    "# Overlay topological defect regions in degrees\n",
    "plt.axvspan(18, 90, color='blue', alpha=0.3, label='Textures (θ ≈ 18° to 90°)')\n",
    "plt.axvspan(36, 90, color='red', alpha=0.3, label='Domain Walls (θ ≈ 36° to 90°)')\n",
    "plt.axvspan(1, 4, color='green', alpha=0.2, label='Cosmic Strings (θ ≈ 1° to 4°)')\n",
    "plt.axvspan(60, 90, color='orange', alpha=0.4, label='Monopoles (θ ≈ 60° to 90°)')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(\"CMB Power vs Angular Scale with Topological Defect Regions\")\n",
    "plt.xlabel(\"Angular Scale θ [degrees]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.gca().invert_xaxis()  # Large scales (low ℓ) on left\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2e2cb26eda2e7220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of exaggerated cosmic strings",
   "id": "1f62f590ae13c173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from skimage import draw  # Changed import to use skimage.draw instead of matplotlib.pyplot.draw\n",
    "\n",
    "def simulate_cosmic_strings(map_size, num_strings):\n",
    "    map_data = np.zeros((map_size, map_size))\n",
    "    for _ in range(num_strings):\n",
    "        x = np.random.randint(0, map_size)\n",
    "        y = np.random.randint(0, map_size)\n",
    "        dx = np.random.randint(-map_size//4, map_size//4)\n",
    "        dy = np.random.randint(-map_size//4, map_size//4)\n",
    "        rr, cc = draw.line(x, y, x+dx, y+dy)  # Now using skimage.draw.line\n",
    "        map_data[rr % map_size, cc % map_size] += np.random.choice([-1, 1]) * 100e-6  # ~100 µK jump\n",
    "    return map_data\n",
    "\n",
    "# Simulate and plot\n",
    "map_size = 1000  # Reduced size for better visualization\n",
    "num_strings = 10\n",
    "cosmic_map = simulate_cosmic_strings(map_size, num_strings)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cosmic_map, cmap='RdBu_r')\n",
    "plt.colorbar(label='Temperature fluctuation (K)')\n",
    "plt.title('Simulated Cosmic Strings')\n",
    "plt.show()\n"
   ],
   "id": "127d1aff7138c278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploring the data with Machine Learning Methods\n",
    "\n",
    "In this section, the application of various machine learning techniques to analyze the CMB data and identify potential patterns or anomalies that might be associated with cosmic strings or other phenomena.\n",
    "\n",
    "Ensure the data and necessary libraries are loaded (just means sections below are not reliant on sections above.)\n"
   ],
   "id": "3918d60f92e61230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary ML libraries\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Make sure we have the CMB map loaded\n",
    "# If not already loaded, load it again\n",
    "if 'cmb_map' not in locals():\n",
    "    cmb_map = hp.read_map('data/COM_CMB_IQU-smica_2048_R3.00_full.fits', field=0)\n",
    "    nside = hp.get_nside(cmb_map)\n",
    "    npix = hp.nside2npix(nside)\n",
    "    img = hp.mollview(cmb_map, return_projected_map=True, nest=False, title='CMB Temperature Map', cmap='inferno', xsize=2000, hold=True)\n",
    "    img_cleaned = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"CMB map shape: {cmb_map.shape}\")\n",
    "print(f\"Projected image shape: {img_cleaned.shape}\")\n"
   ],
   "id": "94951ca05a9d602b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Feature Extraction and Dimensionality Reduction\n",
    "\n",
    "First, extracting features from the CMB data and reducing dimensionality to visualize patterns.\n"
   ],
   "id": "4816520929fc27d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Single Cluster",
   "id": "b6ecb64e64110d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For ML analysis, we'll work with the 2D projected map (img_cleaned)\n",
    "# Let's extract patches from the image to use as features\n",
    "\n",
    "def extract_patches(image, patch_size=16, stride=8):\n",
    "    \"\"\"Extract patches from a 2D image with a given stride.\"\"\"\n",
    "    patches = []\n",
    "    positions = []\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            # Skip patches with NaN or inf values\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            if np.isfinite(patch).all() and not np.isnan(patch).any():\n",
    "                # Flatten the patch to a 1D array\n",
    "                patches.append(patch.flatten())\n",
    "                positions.append((i, j))\n",
    "\n",
    "    return np.array(patches), positions\n",
    "\n",
    "# Extract patches from the cleaned image\n",
    "# patch_size = 4 , stride = 2\tVery localized features, fast\tNo texture, easy to overfit noise\n",
    "# patch_size = 8 , stride = 4\tUltra-local\tVery sensitive to noise; good for edge detection\n",
    "# patch_size = 16, stride = 8\tSmall patches, overlapping\tDense coverage, good for texture analysis\n",
    "# patch_size = 32, stride = 32\tLarge, non-overlapping\tCoarse, fewer samples but high information per patch\n",
    "# patch_size = 64, stride = 16\tLarge and overlapping\tHeavy computation; useful for large-scale pattern mining\n",
    "#\n",
    "#\n",
    "# Try three experimental combinations:\n",
    "# Test\tPatch Size\tStride\tUse Case\n",
    "# A\t    16\t        8\t    Balanced: local structure + manageable data\n",
    "# B\t    32\t        16\t    Large structures, good for global coherence\n",
    "# C\t    8\t        4\t    Detect sharp features (e.g., cosmic strings, shocks)\n",
    "patch_size = 8\n",
    "stride = 4\n",
    "patches, positions = extract_patches(img_cleaned, patch_size, stride)\n",
    "\n",
    "print(f\"Extracted {len(patches)} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "# Standardize the patches\n",
    "scaler = StandardScaler()\n",
    "patches_scaled = scaler.fit_transform(patches)\n",
    "\n",
    "# Fit PCA with all components first to inspect explained variance\n",
    "pca_full = PCA()\n",
    "pca_full.fit(patches_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Define your target variance threshold (e.g., 95%)\n",
    "#     target_variance: A pre-defined threshold (e.g., 0.95 for 95%) indicating how much of the\n",
    "#     original data's variability PCA should preserve.\n",
    "#\n",
    "#     Purpose: Balances dimensionality reduction against information loss.\n",
    "target_variance = 0.95\n",
    "\n",
    "# Find the number of components that meet or exceed this threshold\n",
    "n_components = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "print(f\"Number of components to retain {target_variance*100:.0f}% variance: {n_components}\")\n",
    "\n",
    "\n",
    "# Apply PCA again with the optimal number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "patches_pca = pca.fit_transform(patches_scaled)\n",
    "\n",
    "# Reconstruct data with selected components\n",
    "patches_reduced = pca.fit_transform(patches_scaled)\n",
    "patches_reconstructed = pca.inverse_transform(patches_reduced)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = np.mean((patches_scaled - patches_reconstructed) ** 2)\n",
    "print(f\"Reconstruction MSE: {mse:.2e}\")\n",
    "\n",
    "# Optional: plot the explained variance curve with a line at the threshold\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_variance, label='Cumulative Explained Variance')\n",
    "plt.axhline(y=target_variance, color='r', linestyle='--', label=f'{target_variance*100:.0f}% Variance')\n",
    "plt.axvline(x=n_components - 1, color='g', linestyle='--', label=f'{n_components} Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Apply t-SNE for visualization\n",
    "# t-Distributed Stochastic Neighbor Embedding\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "patches_tsne = tsne.fit_transform(patches_pca)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(patches_tsne[:, 0], patches_tsne[:, 1], alpha=0.5, s=5)\n",
    "plt.title('t-SNE Visualization of CMB Patches')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "5645af766ac7a64e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Is This MSE Value Good?\n",
    "\n",
    "Typical MSE Range for CMB Data:\n",
    "\n",
    "Excellent Reconstruction: MSE < 1e-02 (near-perfect preservation of CMB features)\n",
    "\n",
    "Good Reconstruction: MSE ~1e-02 to 1e-01 (retains most cosmological signals)\n",
    "\n",
    "Moderate Reconstruction: MSE ~1e-01 to 5e-01 (some loss of small-scale fluctuations)\n",
    "\n",
    "Poor Reconstruction: MSE > 5e-01 (significant smoothing/feature loss)\n",
    "\n",
    "Aim for MSE < 1e-02 to ensure topology-sensitive features survive dimensionality reduction."
   ],
   "id": "9ea2892a132ffd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if mse < 1e-02:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is excellent for CMB data\")\n",
    "elif mse < 1e-01:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is good for CMB data\")\n",
    "elif mse < 5e-01:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is moderate for CMB data\")\n",
    "else:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is not good for CMB data\")"
   ],
   "id": "25711d4025912981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Combined Clusters",
   "id": "8e95731d5bc79428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# --- PATCH EXTRACTION FUNCTION ---\n",
    "def extract_patches(image, patch_size=16, stride=8):\n",
    "    \"\"\"Extract patches from a 2D image with a given stride.\"\"\"\n",
    "    patches = []\n",
    "    positions = []\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            if np.isfinite(patch).all() and not np.isnan(patch).any():\n",
    "                patches.append(patch.flatten())\n",
    "                positions.append((i, j))\n",
    "\n",
    "    return np.array(patches), positions\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "patch_sizes = [8, 16, 32]  # Multi-scale patch sizes\n",
    "stride = 4\n",
    "target_variance = 0.95\n",
    "\n",
    "# --- COLLECT ALL FEATURES ---\n",
    "all_features = []\n",
    "n_components_dict = {}\n",
    "explained_variances = {}\n",
    "mse_dict = {}\n",
    "\n",
    "pca_models = {}\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "for patch_size in patch_sizes:\n",
    "    print(f\"\\n🔍 Processing patch size: {patch_size}x{patch_size}\")\n",
    "\n",
    "    # Extract patches\n",
    "    patches, _ = extract_patches(img_cleaned, patch_size, stride)\n",
    "    print(f\"Extracted {len(patches)} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    patches_scaled = scaler.fit_transform(patches)\n",
    "\n",
    "    # PCA to inspect variance\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(patches_scaled)\n",
    "    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "    # Determine components needed\n",
    "    n_components = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "    n_components_dict[patch_size] = n_components\n",
    "    explained_variances[patch_size] = cumulative_variance\n",
    "\n",
    "    print(f\"✅ {n_components} components retain {target_variance*100:.0f}% variance\")\n",
    "\n",
    "    # Apply PCA with optimal components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    patches_pca = pca.fit_transform(patches_scaled)\n",
    "\n",
    "    # Reconstruct from PCA\n",
    "    patches_reconstructed = pca.inverse_transform(patches_pca)\n",
    "\n",
    "    # Compute reconstruction MSE\n",
    "    mse = np.mean((patches_scaled - patches_reconstructed) ** 2)\n",
    "    mse_dict[patch_size] = mse\n",
    "    print(f\"📉 Reconstruction MSE for {patch_size}x{patch_size} patches: {mse:.2e}\")\n",
    "\n",
    "    # Store PCA and scaler for later analysis\n",
    "    pca_models[patch_size] = pca\n",
    "    scalers[patch_size] = scaler\n",
    "\n",
    "    # Save features\n",
    "    all_features.append(patches_pca)\n",
    "\n",
    "\n",
    "# --- COMBINE MULTISCALE FEATURES ---\n",
    "# --- Find common patch count across all scales ---\n",
    "min_len = min(f.shape[0] for f in all_features)\n",
    "print(f\"\\n🔧 Truncating all features to {min_len} samples for alignment\")\n",
    "\n",
    "# Truncate all feature arrays to the minimum number of patches\n",
    "all_features_trimmed = [f[:min_len] for f in all_features]\n",
    "\n",
    "# Concatenate safely\n",
    "combined_features = np.concatenate(all_features_trimmed, axis=1)\n",
    "print(f\"🔗 Combined feature shape: {combined_features.shape}\")\n",
    "\n",
    "\n",
    "# --- OPTIONAL: Plot explained variance curves for each scale ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "for size in patch_sizes:\n",
    "    plt.plot(explained_variances[size], label=f'{size}x{size}')\n",
    "plt.axhline(y=target_variance, color='r', linestyle='--', label='Target Variance')\n",
    "plt.title('Cumulative Explained Variance per Patch Size')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- APPLY t-SNE ON COMBINED FEATURES ---\n",
    "print(\"\\n🎯 Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate='auto')\n",
    "features_tsne = tsne.fit_transform(combined_features)\n",
    "\n",
    "# --- PLOT T-SNE ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(features_tsne[:, 0], features_tsne[:, 1], alpha=0.5, s=5)\n",
    "plt.title('t-SNE of Multi-Scale PCA-Reduced CMB Patches')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- PRINT MSE SUMMARY ---\n",
    "print(\"\\n📊 MSE Summary by Patch Size:\")\n",
    "for size in patch_sizes:\n",
    "    print(f\"  - {size}x{size}: MSE = {mse_dict[size]:.2e}\")\n"
   ],
   "id": "755b29a505e4db26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Is This MSE Value Good?\n",
    "\n",
    "Typical MSE Range for CMB Data:\n",
    "\n",
    "Excellent Reconstruction: MSE < 1e-02 (near-perfect preservation of CMB features)\n",
    "\n",
    "Good Reconstruction: MSE ~1e-02 to 1e-01 (retains most cosmological signals)\n",
    "\n",
    "Moderate Reconstruction: MSE ~1e-01 to 5e-01 (some loss of small-scale fluctuations)\n",
    "\n",
    "Poor Reconstruction: MSE > 5e-01 (significant smoothing/feature loss)\n",
    "\n",
    "Aim for MSE < 1e-02 to ensure topology-sensitive features survive dimensionality reduction."
   ],
   "id": "9cb762c98ba5537a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Evaluate Reconstruction Quality ---\n",
    "print(\"📌 MSE Quality Assessment per Patch Size:\")\n",
    "for size in patch_sizes:\n",
    "    mse = mse_dict[size]\n",
    "    if mse < 1e-02:\n",
    "        print(f\"  ✅ {size}x{size}: Reconstruction MSE = {mse:.2e} is **excellent** for CMB data\")\n",
    "    elif mse < 1e-01:\n",
    "        print(f\"  👍 {size}x{size}: Reconstruction MSE = {mse:.2e} is **good** for CMB data\")\n",
    "    elif mse < 5e-01:\n",
    "        print(f\"  ⚠️ {size}x{size}: Reconstruction MSE = {mse:.2e} is **moderate** for CMB data\")\n",
    "    else:\n",
    "        print(f\"  ❌ {size}x{size}: Reconstruction MSE = {mse:.2e} is **not good** for CMB data\")\n"
   ],
   "id": "60d17748bbdee0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Unsupervised Learning: Clustering Analysis\n",
    "\n",
    "Now we'll apply clustering algorithms to identify groups of similar patterns in the CMB data.\n"
   ],
   "id": "732ef428e849ab2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Dynamic Optimization of Clustering Parameters\n",
    "\n",
    "To find the optimal clustering of the CMB data, a dynamic approach is used that:\n",
    "\n",
    "1. Tries different numbers of clusters (from 3 to 50)\n",
    "2. For each number of clusters, runs K-means multiple times with different random initializations\n",
    "3. Calculates the silhouette score for each clustering attempt\n",
    "4. Selects the clustering with the highest silhouette score\n",
    "\n",
    "This approach helps find the best clustering configuration automatically, rather than manually tuning parameters. The silhouette score measures how well-separated the clusters are, with higher scores indicating better-defined clusters.\n"
   ],
   "id": "8c446d0e0e97248e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Expected Silhouette Score Range for CMB Data\n",
    "\n",
    "    Good clustering:\n",
    "\n",
    "    0.5 - 1.0 → Strong evidence of cluster structure (rare for CMB unless studying clear anomalies like cold spots or non-Gaussian features).\n",
    "\n",
    "    0.3 - 0.5 → Reasonable separation (may indicate subtle non-Gaussianities or foreground contamination).\n",
    "\n",
    "    Ambiguous clustering:\n",
    "\n",
    "    0.1 - 0.3 → Weak structure (common for Gaussian CMB fluctuations; clusters may be artificial).\n",
    "\n",
    "    No meaningful clusters:\n",
    "\n",
    "    ≤ 0.1 or negative → Likely noise or overfitting (common if forcing clusters on Gaussian random fields).\n",
    "\n",
    "Key Insight:\n",
    "CMB is mostly Gaussian, so high silhouette scores are unexpected unless you're targeting specific anomalies or foregrounds. A score of 0.2-0.4 might be the realistic upper limit for most analyses.\n"
   ],
   "id": "7062a756daaf774b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Single Cluster",
   "id": "810be95ce2be9d70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to run K-means clustering multiple times and find the best silhouette score\n",
    "def find_best_kmeans(data, min_clusters=2, max_clusters=10, n_attempts=10):\n",
    "    \"\"\"\n",
    "    Run K-means clustering multiple times with different parameters to find the best silhouette score.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The data to cluster\n",
    "    min_clusters : int\n",
    "        Minimum number of clusters to try\n",
    "    max_clusters : int\n",
    "        Maximum number of clusters to try\n",
    "    n_attempts : int\n",
    "        Number of random initializations to try for each number of clusters\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_kmeans : KMeans\n",
    "        The best KMeans model\n",
    "    best_labels : array\n",
    "        The cluster labels from the best model\n",
    "    best_n_clusters : int\n",
    "        The number of clusters in the best model\n",
    "    best_score : float\n",
    "        The silhouette score of the best model\n",
    "    \"\"\"\n",
    "    best_score = -1\n",
    "    current_best_kmeans = -100\n",
    "    best_kmeans = None\n",
    "    best_labels = None\n",
    "    best_n_clusters = 0\n",
    "    break_score = 0\n",
    "\n",
    "    # Try different numbers of clusters\n",
    "    for n_clusters in range(min_clusters, max_clusters + 1):\n",
    "        print(f\"Trying {n_clusters} clusters...\")\n",
    "        # Try multiple random initializations for each number of clusters\n",
    "        for attempt in range(n_attempts):\n",
    "            # Initialize and fit KMeans\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=attempt)\n",
    "            labels = kmeans.fit_predict(data)\n",
    "            # Calculate silhouette score\n",
    "            score = silhouette_score(data, labels)\n",
    "            current_best_kmeans = score\n",
    "            print(f\"  Attempt {attempt+1}/{n_attempts}: Silhouette Score = {score:.9f}\")\n",
    "\n",
    "            # Update best model if this one is better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_kmeans = kmeans\n",
    "                best_labels = labels\n",
    "                best_n_clusters = n_clusters\n",
    "                print(f\"  New best score: {best_score:.9f} with {best_n_clusters} clusters\")\n",
    "            if score <= 0.1:\n",
    "                print(f\"  No meaningful clusters found for {n_clusters} clusters\")\n",
    "                break\n",
    "            if score <= best_score-0.05:\n",
    "                print(f\"  No improvement in silhouette score for {n_clusters} clusters\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nBest clustering: {best_n_clusters} clusters with silhouette score {best_score:.9f}\")\n",
    "    return best_kmeans, best_labels, best_n_clusters, best_score\n",
    "\n",
    "# Apply K-means clustering with multiple attempts to find the best silhouette score\n",
    "min_clusters = 3\n",
    "max_clusters = 10\n",
    "n_attempts = 5\n",
    "print(f\"Finding best K-means clustering (trying {min_clusters}-{max_clusters} clusters, {n_attempts} attempts each)...\")\n",
    "best_kmeans, cluster_labels, best_n_clusters, best_silhouette = find_best_kmeans(\n",
    "    patches_pca, min_clusters=min_clusters, max_clusters=max_clusters, n_attempts=n_attempts\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"t-SNE shape: {patches_tsne.shape}\")\n",
    "print(f\"Cluster labels shape: {cluster_labels.shape}\")\n"
   ],
   "id": "c32d1c7c10cce2cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "patches_tsne_clustered = patches_tsne[:len(cluster_labels)]\n",
    "\n",
    "# Visualize clusters in t-SNE space\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(patches_tsne_clustered[:, 0], patches_tsne_clustered[:, 1],\n",
    "                      c=cluster_labels, cmap='viridis', alpha=0.7, s=10)\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'K-means Clustering (k={best_n_clusters}, Silhouette={best_silhouette:.3f})')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "# Calculate silhouette score to evaluate clustering quality\n",
    "#     Higher Silhouette Score → Better clustering quality\n",
    "#     (points are correctly assigned to tight, well-separated clusters).\n",
    "#\n",
    "#     Lower Silhouette Score → Worse clustering quality\n",
    "#     (points may be misassigned or clusters overlap).\n",
    "# 0.2-0.4 would be a good score for CMB data due to the nature of it\n",
    "print(f\"Best Silhouette Score: {best_silhouette:.3f} with {best_n_clusters} clusters\")"
   ],
   "id": "2aa5ed363ce77a5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize cluster centers in image space\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(best_n_clusters):\n",
    "    plt.subplot(1, best_n_clusters, i+1)\n",
    "    # Get the center of the cluster in original space\n",
    "    center = pca.inverse_transform(best_kmeans.cluster_centers_[i])\n",
    "    center = scaler.inverse_transform([center])[0]\n",
    "    # Reshape to patch size\n",
    "    center = center.reshape(patch_size, patch_size)\n",
    "    plt.imshow(center, cmap='inferno')\n",
    "    plt.title(f'Cluster {i}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Map clusters back to the original image\n",
    "cluster_map = np.zeros(img_cleaned.shape)\n",
    "cluster_count = np.zeros(img_cleaned.shape)\n",
    "\n",
    "for (i, j), label in zip(positions, cluster_labels):\n",
    "    cluster_map[i:i+patch_size, j:j+patch_size] += label\n",
    "    cluster_count[i:i+patch_size, j:j+patch_size] += 1\n",
    "\n",
    "# Average the cluster labels where patches overlap\n",
    "mask = cluster_count > 0\n",
    "cluster_map[mask] /= cluster_count[mask]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cluster_map, cmap='viridis')\n",
    "plt.title(f'Cluster Map of CMB Data (k={best_n_clusters}, Silhouette={best_silhouette:.3f})')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ],
   "id": "38bf94b1e4d4f2d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check variance retention\n",
    "# Aim for ≥95% for CMB (unlike images, where 80-90% may suffice).\n",
    "print(f\"Variance retained: {np.sum(pca.explained_variance_ratio_):.2%}\")"
   ],
   "id": "f40a1aa650fb1318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## For Combined Clusters",
   "id": "7727615cdbefb6e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- PREP: Store clustering results ---\n",
    "best_kmeans_models = {}\n",
    "cluster_labels_dict = {}\n",
    "retained_variance_dict = {}\n",
    "\n",
    "# Choose patch sizes for fusion\n",
    "selected_patch_sizes = patch_sizes  # example: use 4x4 and 16x16\n",
    "\n",
    "# Apply silhouette-optimized KMeans per selected patch size\n",
    "for size in selected_patch_sizes:\n",
    "    print(f\"\\n🔍 Running KMeans with silhouette scoring on {size}x{size} patches\")\n",
    "\n",
    "    features = all_features[patch_sizes.index(size)][:min_len]  # truncate to match others\n",
    "    best_kmeans, labels, n_clusters, silhouette = find_best_kmeans(\n",
    "        features, min_clusters=3, max_clusters=5, n_attempts=3\n",
    "    )\n",
    "\n",
    "    best_kmeans_models[size] = best_kmeans\n",
    "    cluster_labels_dict[size] = labels\n",
    "    print(f\"✅ Best k={n_clusters} with silhouette score={silhouette:.3f}\")\n",
    "\n",
    "# --- COMBINE MULTISCALE LABELS INTO META-FEATURE VECTOR ---\n",
    "print(\"\\n🔗 Building meta-feature vector from cluster labels\")\n",
    "meta_features = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "print(f\"Meta-feature shape: {meta_features.shape}\")\n",
    "\n",
    "# --- APPLY ANOMALY DETECTION TO META-FEATURE VECTOR ---\n",
    "print(\"\\n🌌 Running Isolation Forest on cluster fusion features\")\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso.fit_predict(meta_features)\n",
    "\n",
    "# Convert to anomaly score (1 = normal, -1 = anomaly → flip)\n",
    "anomaly_scores = -1 * anomaly_labels\n",
    "print(f\"Anomaly scores: {np.unique(anomaly_scores, return_counts=True)}\")\n"
   ],
   "id": "61c0f80237e0eb19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- VISUALIZE IN T-SNE SPACE ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(features_tsne[:len(anomaly_scores), 0], features_tsne[:len(anomaly_scores), 1],\n",
    "            c=anomaly_scores, cmap='coolwarm', alpha=0.7, s=10)\n",
    "plt.title('Anomaly Detection from Multi-Scale Cluster Fusion')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5d893c517182604c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply K-means clustering with multiple attempts to find the best silhouette score\n",
    "min_clusters = 3\n",
    "max_clusters = 10\n",
    "n_attempts = 5\n",
    "\n",
    "print(f\"\\n🔎 Finding best K-means clustering on multi-scale features ({min_clusters}-{max_clusters} clusters, {n_attempts} attempts each)...\")\n",
    "\n",
    "# Use the multi-scale features for clustering\n",
    "best_kmeans, cluster_labels, best_n_clusters, best_silhouette = find_best_kmeans(\n",
    "    combined_features, min_clusters=min_clusters, max_clusters=max_clusters, n_attempts=n_attempts\n",
    ")\n",
    "\n",
    "# Visualize clusters in t-SNE space (already computed from combined_features)\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'K-means Clustering on Multi-Scale Features (k={best_n_clusters}, Silhouette={best_silhouette:.3f})')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# --- Track PCA variance retained ---\n",
    "n_components_used = n_components_dict[size]\n",
    "retained_variance = explained_variances[size][n_components_used - 1]\n",
    "retained_variance_dict[size] = retained_variance\n",
    "\n",
    "# Report silhouette quality (typical good range is 0.2–0.4 for noisy CMB)\n",
    "print(f\"📈 Best Silhouette Score: {best_silhouette:.3f} | {best_n_clusters} clusters | Variance Retained={retained_variance:.3f}\" )\n"
   ],
   "id": "6c4da5eaff0d2269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Alternate Method",
   "id": "be434ef5e46d7817"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- PREP: Store clustering results ---\n",
    "best_kmeans_models = {}\n",
    "cluster_labels_dict = {}\n",
    "silhouette_scores_per_size = {}\n",
    "entropy_scores = {}\n",
    "\n",
    "# --- CHOOSE PATCH SIZES FOR META-FUSION ---\n",
    "selected_patch_sizes = patch_sizes  # or e.g., [4, 8, 16]\n",
    "\n",
    "# --- SILHOUETTE-OPTIMIZED KMEANS CLUSTERING ---\n",
    "for size in selected_patch_sizes:\n",
    "    print(f\"\\n🔍 Running KMeans on {size}x{size} features with silhouette scoring\")\n",
    "\n",
    "    features = all_features[patch_sizes.index(size)][:min_len]  # align patch count\n",
    "\n",
    "    sil_scores = []\n",
    "    all_models = []\n",
    "    all_labels = []\n",
    "\n",
    "    for k in range(3, 21):\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = km.fit_predict(features)\n",
    "        sil = silhouette_score(features, labels)\n",
    "        sil_scores.append(sil)\n",
    "        all_models.append(km)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    best_idx = int(np.argmax(sil_scores))\n",
    "    best_k = best_idx + 3\n",
    "    best_kmeans = all_models[best_idx]\n",
    "    best_labels = all_labels[best_idx]\n",
    "    best_score = sil_scores[best_idx]\n",
    "\n",
    "    best_kmeans_models[size] = best_kmeans\n",
    "    cluster_labels_dict[size] = best_labels\n",
    "    silhouette_scores_per_size[size] = best_score\n",
    "\n",
    "    # Compute entropy of cluster label distribution\n",
    "    label_counts = np.bincount(best_labels)\n",
    "    probs = label_counts / np.sum(label_counts)\n",
    "    ent = entropy(probs)\n",
    "    entropy_scores[size] = ent\n",
    "\n",
    "    print(f\"✅ Best k={best_k} | Silhouette={best_score:.3f} | Entropy={ent:.3f}\")\n",
    "\n",
    "    # Plot silhouette curve\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(range(3, 21), sil_scores, marker='o')\n",
    "    plt.title(f'Silhouette Scores for {size}x{size}')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- META-FEATURE COMBINATION ---\n",
    "print(\"\\n🔗 Combining cluster labels into meta-feature vectors\")\n",
    "meta_features = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "print(f\"Meta-feature shape: {meta_features.shape}\")\n",
    "\n",
    "# --- META-FEATURE DIAGNOSTICS ---\n",
    "\n",
    "# Correlation Matrix\n",
    "print(\"\\n📊 Label Correlation Matrix (Pearson):\")\n",
    "corr_matrix = np.corrcoef(meta_features.T)\n",
    "print(np.round(corr_matrix, 3))\n",
    "\n",
    "# Entropy of each label column\n",
    "print(\"\\n🧠 Entropy of Cluster Label Distributions:\")\n",
    "for size in selected_patch_sizes:\n",
    "    print(f\"  - {size}x{size}: Entropy = {entropy_scores[size]:.3f}\")\n",
    "\n",
    "# --- ISOLATION FOREST FOR ANOMALY DETECTION ---\n",
    "print(\"\\n🌌 Running Isolation Forest on meta-cluster features...\")\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso.fit_predict(meta_features)\n",
    "\n",
    "# Convert -1 to 1 (anomaly), 1 to 0 (normal)\n",
    "anomaly_scores = -1 * (anomaly_labels - 1) // 2\n",
    "\n",
    "# Summary\n",
    "unique_vals, counts = np.unique(anomaly_scores, return_counts=True)\n",
    "print(f\"🧭 Anomaly score distribution: {dict(zip(unique_vals, counts))} (1 = Anomaly)\")\n",
    "\n",
    "# --- T-SNE FOR ANOMALY VISUALIZATION ---\n",
    "print(\"\\n🧪 Visualizing anomalies in t-SNE space...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "meta_tsne = tsne.fit_transform(meta_features)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(meta_tsne[:, 0], meta_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', s=10, alpha=0.7)\n",
    "plt.title('t-SNE of Meta-Cluster Features (Anomaly Detection)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Anomaly Score (1 = Anomaly)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "7323938f9f1b888d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "🧠 How to Use This Practically\n",
    "\n",
    "(max Entropy is measured as log(k) and Silhoutte from 0.0-1.0)\n",
    "\n",
    "    Silhouette: Measures cluster separation and compactness.\n",
    "\n",
    "    Entropy: Measures cluster balance.\n",
    "\n",
    "So, ideally:\n",
    "\n",
    "    ✅ High silhouette and high entropy → good clusters, well-separated, evenly sized.\n",
    "\n",
    "    ❌ High silhouette, low entropy → strong separation but one cluster dominates.\n",
    "\n",
    "    ❌ Low silhouette, high entropy → evenly sized clusters but poorly separated.\n",
    "\n",
    "    ❌ Both low → trash clustering."
   ],
   "id": "e5a1580e3954ade4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- CHECK: Silhouette scores for each selected patch size ---\n",
    "print(\"\\n📈 Silhouette Scores per Patch Size:\")\n",
    "for size in selected_patch_sizes:\n",
    "    sil = silhouette_scores_per_size.get(size, None)\n",
    "    if sil is not None:\n",
    "        print(f\"  - {size}x{size}: Silhouette = {sil:.3f}\")\n",
    "    else:\n",
    "        print(f\"  - {size}x{size}: ❌ Not found\")\n",
    "\n",
    "# --- CHECK: Entropy of cluster label distributions ---\n",
    "print(\"\\n🧠 Entropy of Cluster Label Distributions:\")\n",
    "for size in selected_patch_sizes:\n",
    "    labels = cluster_labels_dict[size]\n",
    "    counts = np.bincount(labels)\n",
    "    probs = counts / np.sum(counts)\n",
    "    ent = entropy(probs)\n",
    "    print(f\"  - {size}x{size}: Entropy = {ent:.3f}\")\n",
    "\n",
    "# --- CHECK: Label correlation matrix ---\n",
    "print(\"\\n🔗 Correlation Matrix Between Cluster Label Sets:\")\n",
    "\n",
    "# Create matrix from all selected label vectors\n",
    "label_matrix = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(label_matrix.T)\n",
    "\n",
    "# Display as matrix\n",
    "print(\"    Patch Sizes:\", selected_patch_sizes)\n",
    "print(np.round(corr_matrix, 3))\n",
    "\n",
    "# Optional: visualize the correlation matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.xticks(range(len(selected_patch_sizes)), [f'{s}x{s}' for s in selected_patch_sizes])\n",
    "plt.yticks(range(len(selected_patch_sizes)), [f'{s}x{s}' for s in selected_patch_sizes])\n",
    "plt.title(\"Cluster Label Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "bcd54c9869c9231f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Misc.",
   "id": "861245fb285d48d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare to UMAP (often more stable)\n",
    "from umap import UMAP\n",
    "umap_emb = UMAP(n_components=2).fit_transform(patches_pca)"
   ],
   "id": "dbd8e2f7382c527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Overlay clusters on a CMB map\n",
    "plt.imshow(cluster_map, cmap='viridis', alpha=0.5)\n",
    "plt.imshow(cmb_map, cmap='inferno', alpha=0.5)"
   ],
   "id": "99144d40a0595806",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "labels = clusterer.fit_predict(patches_pca)"
   ],
   "id": "f22305733ef60d95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inertias = []\n",
    "for k in range(min_clusters, max_clusters+1):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(patches_pca)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(min_clusters, max_clusters+1), inertias, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ],
   "id": "f0b2e6f2a8bcbe41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check cluster sizes\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Visualize problematic clusters\n",
    "problem_cluster = 0  # Example\n",
    "problem_indices = np.where(cluster_labels == problem_cluster)[0]\n",
    "plt.scatter(patches_tsne[problem_indices, 0],\n",
    "           patches_tsne[problem_indices, 1])\n",
    "plt.title(f'Problematic Cluster {problem_cluster}')\n",
    "plt.show()"
   ],
   "id": "c413db825255872a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Anomaly Detection\n",
    "\n",
    "Using Isolation Forest to detect anomalies in the CMB data that might correspond to cosmic strings or other interesting features.\n"
   ],
   "id": "5b4933f658f780f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Singular Clusters",
   "id": "5092f045520fa9e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply Isolation Forest for anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_scores = iso_forest.fit_predict(patches_pca)\n",
    "\n",
    "# Convert to anomaly score (higher = more anomalous)\n",
    "anomaly_scores = -1 * anomaly_scores  # -1 becomes +1 (anomaly), 1 becomes -1 (normal)\n",
    "print(f\"Anomaly Score Range: {anomaly_scores.min()} to {anomaly_scores.max()}\")\n",
    "\n",
    "# Visualize anomalies in t-SNE space\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(patches_tsne[:, 0], patches_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Anomaly Score')\n",
    "plt.title('Anomaly Detection in CMB Patches')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Map anomaly scores back to the original image\n",
    "anomaly_map = np.zeros(img_cleaned.shape)\n",
    "anomaly_count = np.zeros(img_cleaned.shape)\n",
    "\n",
    "for (i, j), score in zip(positions, anomaly_scores):\n",
    "    anomaly_map[i:i+patch_size, j:j+patch_size] += score\n",
    "    anomaly_count[i:i+patch_size, j:j+patch_size] += 1\n",
    "\n",
    "# Average the anomaly scores where patches overlap\n",
    "mask = anomaly_count > 0\n",
    "anomaly_map[mask] /= anomaly_count[mask]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(anomaly_map, cmap='coolwarm')\n",
    "plt.title('Anomaly Map of CMB Data (Potential Cosmic String Candidates)')\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.show()\n"
   ],
   "id": "6cb05b2ec54aa324",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use 8x8 patch size for mapping (or adjust to your preference)\n",
    "chosen_patch_size = 8\n",
    "_, positions = extract_patches(img_cleaned, patch_size=chosen_patch_size, stride=stride)\n",
    "\n",
    "# Make sure anomaly scores are the same length as positions\n",
    "min_len = min(len(positions), len(anomaly_scores))\n",
    "positions = positions[:min_len]\n",
    "anomaly_scores = anomaly_scores[:min_len]\n",
    "\n",
    "# --- Build anomaly map ---\n",
    "anomaly_map = np.zeros_like(img_cleaned)\n",
    "anomaly_count = np.zeros_like(img_cleaned)\n",
    "\n",
    "for (i, j), score in zip(positions, anomaly_scores):\n",
    "    anomaly_map[i:i+chosen_patch_size, j:j+chosen_patch_size] += score\n",
    "    anomaly_count[i:i+chosen_patch_size, j:j+chosen_patch_size] += 1\n",
    "\n",
    "# Average overlapping patches\n",
    "mask = anomaly_count > 0\n",
    "anomaly_map[mask] /= anomaly_count[mask]\n",
    "\n",
    "# Optional: clip for visualization clarity\n",
    "vmin, vmax = np.percentile(anomaly_map[mask], [5, 95])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(anomaly_map, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.title('CMB Anomaly Map (Potential Topological Signatures)')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "3b2f798bbd402144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show top N anomalies\n",
    "N = 10\n",
    "top_indices = np.argsort(anomaly_scores)[-N:]\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for idx, i in enumerate(top_indices):\n",
    "    patch = scaler.inverse_transform(pca.inverse_transform(combined_features[i]))[:chosen_patch_size**2]\n",
    "    patch_img = patch.reshape(chosen_patch_size, chosen_patch_size)\n",
    "    plt.subplot(1, N, idx + 1)\n",
    "    plt.imshow(patch_img, cmap='inferno')\n",
    "    plt.title(f'Anomaly {idx+1}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Top Anomalous Patches (Multi-scale features)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "c6c6bf9c0ea6ac3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Multiple Clusters",
   "id": "11ec13c887c3f9a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# --- Apply Isolation Forest on combined multi-scale features ---\n",
    "print(\"\\n🌌 Running Isolation Forest for anomaly detection...\")\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(combined_features)\n",
    "anomaly_scores = iso_forest.decision_function(combined_features)  # Higher = more normal\n",
    "\n",
    "# Invert for easier interpretation (higher = more anomalous)\n",
    "anomaly_scores = -anomaly_scores\n",
    "print(f\"Anomaly score range: {anomaly_scores.min():.4f} to {anomaly_scores.max():.4f}\")\n",
    "\n",
    "# --- Visualize in t-SNE space ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Anomaly Score')\n",
    "plt.title('t-SNE Projection with Anomaly Scores')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "3620069ead4198e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Alternative Method",
   "id": "964970abd1b31927"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# --- ISOLATION FOREST: Anomaly Detection on Meta-Cluster Features ---\n",
    "print(\"\\n🌌 Running Isolation Forest on meta-cluster labels...\")\n",
    "\n",
    "# Build meta-feature matrix from cluster label assignments\n",
    "meta_features = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "print(f\"Meta-feature shape: {meta_features.shape} (samples × {len(selected_patch_sizes)} features)\")\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_labels = iso_forest.fit_predict(meta_features)\n",
    "\n",
    "# Convert: -1 → 1 (anomaly), 1 → 0 (normal)\n",
    "anomaly_scores = -1 * (iso_labels - 1) // 2\n",
    "\n",
    "# --- Summary Stats ---\n",
    "unique_vals, counts = np.unique(anomaly_scores, return_counts=True)\n",
    "summary = dict(zip(unique_vals, counts))\n",
    "print(f\"🧭 Anomaly label distribution: {summary} (1 = Anomaly, 0 = Normal)\")\n",
    "\n",
    "# --- OPTIONAL: t-SNE Visualization of Anomaly Detection ---\n",
    "print(\"\\n🧪 Visualizing anomalies using t-SNE...\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "meta_tsne = tsne.fit_transform(meta_features)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(meta_tsne[:, 0], meta_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', s=12, alpha=0.7)\n",
    "plt.title('t-SNE of Meta-Cluster Features (Isolation Forest Anomalies)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Anomaly Score (1 = Anomaly)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "279200fd26a08672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features (principal components) are most important for distinguishing clusters and anomalies.\n"
   ],
   "id": "697df93ca9035124"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Singular Clusters",
   "id": "ec24efdb92931941"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyze the most important principal components\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_components), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Importance of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analyze the first few principal components\n",
    "n_display = min(5, n_components)\n",
    "plt.figure(figsize=(15, 3*n_display))\n",
    "for i in range(n_display):\n",
    "    plt.subplot(n_display, 1, i+1)\n",
    "    component = pca.components_[i].reshape(1, -1)\n",
    "    component_image = scaler.inverse_transform(component)[0].reshape(patch_size, patch_size)\n",
    "    plt.imshow(component_image, cmap='coolwarm')\n",
    "    plt.title(f'Principal Component {i+1}')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "80b1b9345b2ea876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Multiple Clusters",
   "id": "1c21f80abbe9690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Bar Plot: Explained Variance per Scale ---\n",
    "for size in patch_sizes:\n",
    "    pca = pca_models[size]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "    plt.title(f'Explained Variance by PCA Components — {size}x{size} patches')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "adafe0ebc7446216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visualize Top N PCA Components ---\n",
    "n_display = 5\n",
    "\n",
    "for size in patch_sizes:\n",
    "    pca = pca_models[size]\n",
    "    scaler = scalers[size]\n",
    "    components = pca.components_\n",
    "    patch_len = size * size\n",
    "\n",
    "    print(f\"\\n🔬 Visualizing top {n_display} PCA components for {size}x{size} patches\")\n",
    "\n",
    "    plt.figure(figsize=(15, 3 * n_display))\n",
    "    for i in range(min(n_display, components.shape[0])):\n",
    "        # Get component and inverse-scale it\n",
    "        component = components[i].reshape(1, -1)\n",
    "        restored = scaler.inverse_transform(pca.inverse_transform(component))[0]\n",
    "        patch = restored[:patch_len].reshape(size, size)\n",
    "\n",
    "        plt.subplot(n_display, 1, i + 1)\n",
    "        plt.imshow(patch, cmap='coolwarm')\n",
    "        plt.colorbar()\n",
    "        plt.title(f'{size}x{size} Patch — Principal Component {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "4c0a54d25e1beb33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Correlation with Edge Detection\n",
    "\n",
    "Let's compare our ML-based anomaly detection with the edge detection performed earlier to see if they identify similar features.\n"
   ],
   "id": "fb828f9cb59926f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Singular Cluster",
   "id": "ddf06ce857e2ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure we have the edge detection results\n",
    "if 'edges' not in locals():\n",
    "    edges = ndimage.sobel(img_cleaned)\n",
    "\n",
    "# Normalize both maps for comparison\n",
    "edges_norm = (edges - np.min(edges)) / (np.max(edges) - np.min(edges))\n",
    "anomaly_norm = (anomaly_map - np.min(anomaly_map)) / (np.max(anomaly_map) - np.min(anomaly_map))\n",
    "\n",
    "# Calculate correlation between edge detection and anomaly detection\n",
    "valid_mask = ~np.isnan(edges_norm) & ~np.isnan(anomaly_norm)\n",
    "correlation = np.corrcoef(edges_norm[valid_mask].flatten(), anomaly_norm[valid_mask].flatten())[0, 1]\n",
    "print(f\"Correlation between edge detection and anomaly detection: {correlation:.3f}\")\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(edges_norm, cmap='inferno')\n",
    "plt.title('Edge Detection')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edges_norm, cmap='inferno', alpha=0.7)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm', alpha=0.3)\n",
    "plt.title('Edge + Anomaly Overlay')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "d7c18ae0c8994923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Multiple Clusters",
   "id": "60c8b32b64a824a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "# --- ENSURE EDGE DETECTION EXISTS ---\n",
    "if 'edges' not in locals():\n",
    "    print(\"🔍 Computing Sobel edge detection...\")\n",
    "    edges = ndimage.sobel(img_cleaned)\n",
    "\n",
    "# --- NORMALIZE MAPS FOR COMPARISON ---\n",
    "def normalize_map(x):\n",
    "    x = np.nan_to_num(x)\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-8)\n",
    "\n",
    "edges_norm = normalize_map(edges)\n",
    "anomaly_norm = normalize_map(anomaly_map)\n",
    "\n",
    "# --- CREATE VALID MASK ---\n",
    "valid_mask = (~np.isnan(edges_norm)) & (~np.isnan(anomaly_norm))\n",
    "edges_flat = edges_norm[valid_mask].flatten()\n",
    "anomaly_flat = anomaly_norm[valid_mask].flatten()\n",
    "\n",
    "# --- CORRELATION CALCULATION ---\n",
    "if edges_flat.size > 0 and anomaly_flat.size > 0:\n",
    "    correlation = np.corrcoef(edges_flat, anomaly_flat)[0, 1]\n",
    "    print(f\"📈 Correlation between edge detection and anomaly detection: {correlation:.3f}\")\n",
    "else:\n",
    "    correlation = np.nan\n",
    "    print(\"⚠️ No valid pixels for correlation.\")\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(edges_norm, cmap='inferno')\n",
    "plt.title('🟠 Edge Detection (Sobel)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm')\n",
    "plt.title('🔵 Anomaly Score (IForest)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edges_norm, cmap='inferno', alpha=0.7)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm', alpha=0.3)\n",
    "plt.title('🎯 Edge + Anomaly Overlay')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a7f7958c6e6892e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a more informative overlay visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot the edge detection map with a semi-transparent inferno colormap\n",
    "plt.imshow(edges_norm, cmap='inferno', alpha=0.7)\n",
    "\n",
    "# Overlay the anomaly map with a semi-transparent coolwarm colormap\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm', alpha=0.5)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Enhanced Edge and Anomaly Overlay')\n",
    "\n",
    "# Add colorbars for both maps\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "ax = plt.gca()\n",
    "divider = make_axes_locatable(ax)\n",
    "\n",
    "# Add colorbar for edges\n",
    "cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar1 = plt.colorbar(plt.cm.ScalarMappable(cmap='inferno'), cax=cax1)\n",
    "cbar1.set_label('Edge Intensity')\n",
    "\n",
    "# Add colorbar for anomaly\n",
    "cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.3)\n",
    "cbar2 = plt.colorbar(plt.cm.ScalarMappable(cmap='coolwarm'), cax=cax2)\n",
    "cbar2.set_label('Anomaly Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show the product of the two maps for comparison\n",
    "overlap_map = edges_norm * anomaly_norm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(overlap_map, cmap='plasma')\n",
    "plt.title('Overlap Map (Edge × Anomaly)')\n",
    "plt.colorbar(label='Overlap Intensity')\n",
    "plt.show()\n"
   ],
   "id": "4d0ab8a38a19ea0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyze overlayed elements and identify outliers\n",
    "print(\"Analyzing overlayed elements and identifying outliers...\")\n",
    "\n",
    "# Calculate statistics for the overlap map\n",
    "overlap_mean = np.mean(overlap_map)\n",
    "overlap_std = np.std(overlap_map)\n",
    "overlap_median = np.median(overlap_map)\n",
    "overlap_min = np.min(overlap_map)\n",
    "overlap_max = np.max(overlap_map)\n",
    "\n",
    "print(f\"Overlap Map Statistics:\")\n",
    "print(f\"  Mean: {overlap_mean:.6f}\")\n",
    "print(f\"  Median: {overlap_median:.6f}\")\n",
    "print(f\"  Standard Deviation: {overlap_std:.6f}\")\n",
    "print(f\"  Min: {overlap_min:.6f}\")\n",
    "print(f\"  Max: {overlap_max:.6f}\")\n",
    "\n",
    "# Define outliers using standard deviation method (Z-score)\n",
    "z_threshold = 2.5  # Points with Z-score > 2.5 are considered outliers (covers ~99% of normal distribution)\n",
    "overlap_z_scores = (overlap_map - overlap_mean) / overlap_std\n",
    "outliers_mask = np.abs(overlap_z_scores) > z_threshold\n",
    "\n",
    "# Count outliers\n",
    "num_outliers = np.sum(outliers_mask)\n",
    "total_pixels = overlap_map.size\n",
    "outlier_percentage = (num_outliers / total_pixels) * 100\n",
    "\n",
    "print(f\"\\nOutlier Analysis (Z-score > {z_threshold}):\")\n",
    "print(f\"  Number of outliers: {num_outliers}\")\n",
    "print(f\"  Percentage of outliers: {outlier_percentage:.2f}%\")\n",
    "\n",
    "# Create a visualization of the outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot the original overlap map\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(overlap_map, cmap='plasma')\n",
    "plt.title('Original Overlap Map')\n",
    "plt.colorbar(label='Overlap Intensity')\n",
    "\n",
    "# Plot the Z-scores\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(overlap_z_scores, cmap='seismic', vmin=-5, vmax=5)\n",
    "plt.title('Z-scores')\n",
    "plt.colorbar(label='Z-score')\n",
    "\n",
    "# Plot the outliers mask\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(outliers_mask, cmap='gray')\n",
    "plt.title(f'Outliers (Z-score > {z_threshold})')\n",
    "plt.colorbar(label='Is Outlier')\n",
    "\n",
    "# Plot the original image with outliers highlighted\n",
    "plt.subplot(2, 2, 4)\n",
    "highlighted_map = np.copy(img_cleaned)\n",
    "highlighted_map = normalize(highlighted_map)  # Normalize for better visualization\n",
    "highlighted_outliers = np.zeros_like(highlighted_map)\n",
    "highlighted_outliers[outliers_mask] = 1\n",
    "\n",
    "plt.imshow(highlighted_map, cmap='gray', alpha=0.7)\n",
    "plt.imshow(highlighted_outliers, cmap='hot', alpha=0.5)\n",
    "plt.title('CMB Map with Outliers Highlighted')\n",
    "plt.colorbar(label='Outlier Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the characteristics of the outliers\n",
    "if num_outliers > 0:\n",
    "    # Extract values of outliers\n",
    "    outlier_values = overlap_map[outliers_mask]\n",
    "\n",
    "    # Calculate statistics for outliers\n",
    "    outlier_mean = np.mean(outlier_values)\n",
    "    outlier_std = np.std(outlier_values)\n",
    "    outlier_median = np.median(outlier_values)\n",
    "    outlier_min = np.min(outlier_values)\n",
    "    outlier_max = np.max(outlier_values)\n",
    "\n",
    "    print(\"\\nOutlier Values Statistics:\")\n",
    "    print(f\"  Mean: {outlier_mean:.6f}\")\n",
    "    print(f\"  Median: {outlier_median:.6f}\")\n",
    "    print(f\"  Standard Deviation: {outlier_std:.6f}\")\n",
    "    print(f\"  Min: {outlier_min:.6f}\")\n",
    "    print(f\"  Max: {outlier_max:.6f}\")\n",
    "\n",
    "    # Histogram of outlier values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(outlier_values, bins=30, alpha=0.7, color='purple')\n",
    "    plt.title('Distribution of Outlier Values')\n",
    "    plt.xlabel('Overlap Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Compare edge and anomaly values at outlier positions\n",
    "    edge_values_at_outliers = edges_norm[outliers_mask]\n",
    "    anomaly_values_at_outliers = anomaly_norm[outliers_mask]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(edge_values_at_outliers, anomaly_values_at_outliers, alpha=0.5, c=outlier_values, cmap='plasma')\n",
    "    plt.colorbar(label='Overlap Intensity')\n",
    "    plt.title('Edge vs Anomaly Values at Outlier Positions')\n",
    "    plt.xlabel('Edge Detection Value')\n",
    "    plt.ylabel('Anomaly Detection Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ],
   "id": "c4b826e0cfd03fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Wavelet-Based Multi-Scale Analysis for Cosmic String Detection\n",
    "\n",
    "Wavelets are particularly well-suited for detecting line-like structures such as cosmic strings because they can capture both spatial and frequency information simultaneously. Unlike Fourier transforms, wavelets provide localization in both space and frequency domains, making them ideal for detecting transient features like cosmic strings.\n"
   ],
   "id": "f00a1c920ac8bf37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import PyWavelets library\n",
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "# Ensure we have the cleaned CMB image\n",
    "if 'img_cleaned' not in locals():\n",
    "    print(\"Loading and cleaning CMB image...\")\n",
    "    cmb_map = hp.read_map('data/COM_CMB_IQU-smica_2048_R3.00_full.fits', field=0)\n",
    "    img = hp.mollview(cmb_map, return_projected_map=True, nest=False, title='CMB Temperature Map', cmap='inferno', xsize=2000, hold=True)\n",
    "    img_cleaned = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Image shape: {img_cleaned.shape}\")\n",
    "\n",
    "# Define wavelet parameters\n",
    "wavelet = 'cgau8'  # Complex Gaussian wavelet (good for detecting edges and line-like features)\n",
    "max_level = 5      # Maximum decomposition level\n",
    "\n",
    "# Perform 2D continuous wavelet transform\n",
    "coeffs = pywt.wavedec2(img_cleaned, wavelet='db4', level=max_level)\n",
    "\n",
    "# Extract approximation and detail coefficients\n",
    "approx = coeffs[0]\n",
    "details = coeffs[1:]\n",
    "\n",
    "# Visualize approximation coefficients\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(approx, cmap='inferno')\n",
    "plt.title(f'Approximation Coefficients (Level {max_level})')\n",
    "plt.colorbar(label='Coefficient Value')\n",
    "plt.show()\n",
    "\n",
    "# Visualize detail coefficients at each level\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, detail_coeffs in enumerate(details):\n",
    "    level = i + 1\n",
    "    # Each detail contains horizontal, vertical, and diagonal coefficients\n",
    "    horizontal, vertical, diagonal = detail_coeffs\n",
    "\n",
    "    # Plot horizontal coefficients (sensitive to vertical edges)\n",
    "    plt.subplot(max_level, 3, 3*i + 1)\n",
    "    plt.imshow(horizontal, cmap='coolwarm')\n",
    "    plt.title(f'Horizontal Detail (Level {level})')\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot vertical coefficients (sensitive to horizontal edges)\n",
    "    plt.subplot(max_level, 3, 3*i + 2)\n",
    "    plt.imshow(vertical, cmap='coolwarm')\n",
    "    plt.title(f'Vertical Detail (Level {level})')\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot diagonal coefficients\n",
    "    plt.subplot(max_level, 3, 3*i + 3)\n",
    "    plt.imshow(diagonal, cmap='coolwarm')\n",
    "    plt.title(f'Diagonal Detail (Level {level})')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Enhance cosmic string detection by focusing on specific scales\n",
    "# Cosmic strings are expected to be more prominent at certain scales\n",
    "enhanced_coeffs = coeffs.copy()\n",
    "\n",
    "# Modify coefficients to enhance line-like structures\n",
    "# Boost mid-level details (levels 2-3) where cosmic strings might be more visible\n",
    "boost_levels = [1, 2]  # 0-indexed\n",
    "boost_factor = 2.0\n",
    "\n",
    "for level in boost_levels:\n",
    "    # Boost horizontal, vertical, and diagonal coefficients\n",
    "    enhanced_coeffs[level+1] = tuple(coeff * boost_factor for coeff in enhanced_coeffs[level+1])\n",
    "\n",
    "# Reconstruct image from enhanced coefficients\n",
    "enhanced_img = pywt.waverec2(enhanced_coeffs, wavelet='db4')\n",
    "\n",
    "# Ensure reconstructed image has same shape as original\n",
    "if enhanced_img.shape != img_cleaned.shape:\n",
    "    enhanced_img = enhanced_img[:img_cleaned.shape[0], :img_cleaned.shape[1]]\n",
    "\n",
    "# Visualize original vs enhanced image\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_cleaned, cmap='inferno')\n",
    "plt.title('Original CMB Image')\n",
    "plt.colorbar(label='Temperature')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(enhanced_img, cmap='inferno')\n",
    "plt.title('Wavelet-Enhanced Image (Cosmic String Detection)')\n",
    "plt.colorbar(label='Temperature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the difference to highlight enhanced features\n",
    "difference = enhanced_img - img_cleaned\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(difference, cmap='coolwarm')\n",
    "plt.title('Enhanced Features (Potential Cosmic Strings)')\n",
    "plt.colorbar(label='Difference')\n",
    "plt.show()\n",
    "\n",
    "# Apply wavelet-based denoising to better isolate cosmic string candidates\n",
    "denoised = denoise_wavelet(img_cleaned, method='BayesShrink', mode='soft', \n",
    "                          wavelet='db8', wavelet_levels=3)\n",
    "\n",
    "# Calculate edge map from denoised image\n",
    "from scipy import ndimage\n",
    "edges_denoised = ndimage.sobel(denoised)\n",
    "edges_denoised_norm = normalize(edges_denoised)\n",
    "\n",
    "# Visualize denoised image and its edges\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(denoised, cmap='inferno')\n",
    "plt.title('Wavelet-Denoised CMB Image')\n",
    "plt.colorbar(label='Temperature')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(edges_denoised_norm, cmap='inferno')\n",
    "plt.title('Edge Detection on Denoised Image')\n",
    "plt.colorbar(label='Edge Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with previous edge detection results\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(edges_norm, cmap='inferno')\n",
    "plt.title('Original Edge Detection')\n",
    "plt.colorbar(label='Edge Intensity')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(edges_denoised_norm, cmap='inferno')\n",
    "plt.title('Wavelet-Based Edge Detection')\n",
    "plt.colorbar(label='Edge Intensity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between original and wavelet-based edge detection\n",
    "valid_mask = ~np.isnan(edges_norm) & ~np.isnan(edges_denoised_norm)\n",
    "correlation = np.corrcoef(edges_norm[valid_mask].flatten(), \n",
    "                         edges_denoised_norm[valid_mask].flatten())[0, 1]\n",
    "print(f\"Correlation between original and wavelet-based edge detection: {correlation:.12f}\")\n"
   ],
   "id": "a5b5258612f9ff62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "🎯 So what’s a \"good\" correlation?\n",
    "Correlation (r)\tInterpretation\tGood for...\n",
    "0.9\tVery similar, low change\tNoise removal only\n",
    "0.7 – 0.9\tMinor enhancement\tPreserving dominant features\n",
    "0.4 – 0.7\tModerate enhancement\tRevealing subtle structures\n",
    "< 0.4\tLarge change\tNew features revealed, risky if noise amplification"
   ],
   "id": "9ca1aaa96675ebce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Directional Analysis of Potential Cosmic Strings\n",
    "\n",
    "Cosmic strings are expected to have specific directional properties. By analyzing the orientation and direction of detected edges, we can identify patterns that might be characteristic of cosmic strings and distinguish them from other features.\n"
   ],
   "id": "55ba05e15fd2f304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage.feature import hog\n",
    "from skimage import feature, color, exposure, transform\n",
    "import cv2\n",
    "\n",
    "# Ensure we have the edge detection results\n",
    "if 'edges_norm' not in locals():\n",
    "    # Normalize edges for visualization\n",
    "    edges_norm = normalize(edges)\n",
    "\n",
    "# Use Hough transform to detect lines in the edge map\n",
    "# First, threshold the edge map to create a binary image\n",
    "edge_threshold = 0.5\n",
    "binary_edges = (edges_norm > edge_threshold).astype(np.uint8)\n",
    "\n",
    "# Apply Hough transform\n",
    "theta = np.linspace(-np.pi/2, np.pi/2, 180)\n",
    "h, theta, d = transform.hough_line(binary_edges, theta=theta)\n",
    "\n",
    "# Visualize the Hough transform\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(binary_edges, cmap='gray')\n",
    "plt.title('Thresholded Edge Map')\n",
    "plt.colorbar(label='Edge Intensity')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.log1p(h), cmap='inferno', aspect='auto',\n",
    "           extent=[np.rad2deg(theta[0]), np.rad2deg(theta[-1]), d[-1], d[0]])\n",
    "plt.title('Hough Transform (Log Scale)')\n",
    "plt.xlabel('Angle (degrees)')\n",
    "plt.ylabel('Distance (pixels)')\n",
    "plt.colorbar(label='Log(votes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find peaks in the Hough transform to identify significant lines\n",
    "hough_threshold = 0.5 * np.max(h)  # Adjust threshold as needed\n",
    "peaks = feature.peak_local_max(h, min_distance=10, threshold_abs=hough_threshold)\n",
    "\n",
    "# Extract angles and distances of detected lines\n",
    "angles = theta[peaks[:, 1]]\n",
    "distances = d[peaks[:, 0]]\n",
    "\n",
    "# Visualize detected lines on the original image\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_cleaned, cmap='inferno')\n",
    "plt.title('Detected Lines (Potential Cosmic Strings)')\n",
    "\n",
    "for angle, dist in zip(angles, distances):\n",
    "    # Calculate endpoints of the line\n",
    "    a = np.cos(angle)\n",
    "    b = np.sin(angle)\n",
    "    x0 = a * dist\n",
    "    y0 = b * dist\n",
    "\n",
    "    # Calculate line endpoints (extending to image boundaries)\n",
    "    x1 = int(x0 + 1000 * (-b))\n",
    "    y1 = int(y0 + 1000 * (a))\n",
    "    x2 = int(x0 - 1000 * (-b))\n",
    "    y2 = int(y0 - 1000 * (a))\n",
    "\n",
    "    # Plot the line\n",
    "    plt.plot([x1, x2], [y1, y2], 'r-', alpha=0.7)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the distribution of line orientations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(np.rad2deg(angles), bins=36, range=(-90, 90))\n",
    "plt.title('Distribution of Line Orientations')\n",
    "plt.xlabel('Angle (degrees)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate Histogram of Oriented Gradients (HOG) for directional analysis\n",
    "# HOG is particularly good at capturing the local orientation of gradients\n",
    "hog_features, hog_image = hog(\n",
    "    img_cleaned, \n",
    "    orientations=8, \n",
    "    pixels_per_cell=(16, 16),\n",
    "    cells_per_block=(1, 1), \n",
    "    visualize=True,\n",
    "    feature_vector=True\n",
    ")\n",
    "\n",
    "# Enhance HOG visualization\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "# Visualize HOG\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_cleaned, cmap='inferno')\n",
    "plt.title('Original CMB Image')\n",
    "plt.colorbar(label='Temperature')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(hog_image_rescaled, cmap='inferno')\n",
    "plt.title('Histogram of Oriented Gradients (HOG)')\n",
    "plt.colorbar(label='Gradient Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze gradient direction using Sobel operators\n",
    "# Calculate gradients in x and y directions\n",
    "grad_x = ndimage.sobel(img_cleaned, axis=1)\n",
    "grad_y = ndimage.sobel(img_cleaned, axis=0)\n",
    "\n",
    "# Calculate gradient magnitude and direction\n",
    "grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "grad_direction = np.arctan2(grad_y, grad_x)\n",
    "\n",
    "# Convert direction to degrees\n",
    "grad_direction_deg = np.rad2deg(grad_direction)\n",
    "\n",
    "# Visualize gradient magnitude and direction\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(grad_magnitude, cmap='inferno')\n",
    "plt.title('Gradient Magnitude')\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(grad_direction_deg, cmap='hsv')\n",
    "plt.title('Gradient Direction (degrees)')\n",
    "plt.colorbar(label='Direction (degrees)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a quiver plot to visualize gradient vectors\n",
    "# Downsample for clarity\n",
    "step = 20\n",
    "y, x = np.mgrid[0:img_cleaned.shape[0]:step, 0:img_cleaned.shape[1]:step]\n",
    "u = grad_x[::step, ::step]\n",
    "v = grad_y[::step, ::step]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_cleaned, cmap='inferno', alpha=0.7)\n",
    "plt.quiver(x, y, u, v, color='white', scale=50, alpha=0.8)\n",
    "plt.title('Gradient Vector Field (Potential Flow Directions)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify regions with consistent gradient direction (potential cosmic strings)\n",
    "# Calculate the local consistency of gradient directions\n",
    "from scipy.ndimage import generic_filter\n",
    "\n",
    "def direction_consistency(x):\n",
    "    # Convert angles to complex numbers and calculate mean\n",
    "    z = np.exp(1j * x)\n",
    "    mean_z = np.mean(z)\n",
    "    # Return the magnitude of the mean (1 = perfectly aligned, 0 = random)\n",
    "    return np.abs(mean_z)\n",
    "\n",
    "# Apply filter to gradient directions with a 5x5 window\n",
    "consistency = generic_filter(grad_direction, direction_consistency, size=5)\n",
    "\n",
    "# Visualize direction consistency\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(consistency, cmap='inferno')\n",
    "plt.title('Gradient Direction Consistency (Higher = More Aligned)')\n",
    "plt.colorbar(label='Consistency (0-1)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combine direction consistency with gradient magnitude to identify potential cosmic strings\n",
    "cosmic_string_score = consistency * normalize(grad_magnitude)\n",
    "\n",
    "# Visualize the cosmic string score\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cosmic_string_score, cmap='inferno')\n",
    "plt.title('Cosmic String Score (Direction Consistency × Gradient Magnitude)')\n",
    "plt.colorbar(label='Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with previous detection methods\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(edges_norm, cmap='inferno')\n",
    "plt.title('Edge Detection')\n",
    "plt.colorbar(label='Edge Intensity')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(edges_denoised_norm, cmap='inferno')\n",
    "plt.title('Wavelet-Based Edge Detection')\n",
    "plt.colorbar(label='Edge Intensity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(cosmic_string_score, cmap='inferno')\n",
    "plt.title('Directional Analysis Score')\n",
    "plt.colorbar(label='Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "408c77ca395ad375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Statistical Significance Testing of Detected Features\n",
    "\n",
    "To assess the significance of the features detected by our various methods, we need to compare them against what would be expected from random Gaussian fluctuations in the CMB. This helps distinguish genuine cosmic string candidates from statistical flukes.\n"
   ],
   "id": "592979d866e7d475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "import healpy as hp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate random Gaussian CMB maps\n",
    "def generate_random_cmb(nside=2048, lmax=2000, seed=None):\n",
    "    \"\"\"Generate a random Gaussian CMB map with the same power spectrum as the observed CMB.\"\"\"\n",
    "    # Set random seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Get power spectrum from the real CMB map\n",
    "    cl = hp.anafast(cmb_map, lmax=lmax)\n",
    "\n",
    "    # Generate random map with the same power spectrum\n",
    "    random_map = hp.synfast(cl, nside, lmax=lmax, verbose=False)\n",
    "\n",
    "    # Project to 2D\n",
    "    random_img = hp.mollview(random_map, return_projected_map=True, \n",
    "                            nest=False, title='Random CMB Map', \n",
    "                            cmap='inferno', xsize=2000, hold=True)\n",
    "\n",
    "    # Clean the map\n",
    "    random_img_cleaned = np.nan_to_num(random_img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    return random_img_cleaned\n",
    "\n",
    "# Generate a set of random CMB maps for comparison\n",
    "n_simulations = 10\n",
    "print(f\"Generating {n_simulations} random CMB simulations...\")\n",
    "random_maps = [generate_random_cmb(seed=i) for i in range(n_simulations)]\n",
    "\n",
    "# Visualize a few random maps\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(min(4, n_simulations)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(random_maps[i], cmap='inferno')\n",
    "    plt.title(f'Random CMB Simulation #{i+1}')\n",
    "    plt.colorbar(label='Temperature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to calculate feature metrics for a given map\n",
    "def calculate_feature_metrics(img):\n",
    "    \"\"\"Calculate various feature metrics for a given map.\"\"\"\n",
    "    # Edge detection\n",
    "    edges = ndimage.sobel(img)\n",
    "    edges_norm = normalize(edges)\n",
    "\n",
    "    # Gradient direction analysis\n",
    "    grad_x = ndimage.sobel(img, axis=1)\n",
    "    grad_y = ndimage.sobel(img, axis=0)\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    grad_direction = np.arctan2(grad_y, grad_x)\n",
    "\n",
    "    # Direction consistency (5x5 window)\n",
    "    def direction_consistency(x):\n",
    "        z = np.exp(1j * x)\n",
    "        mean_z = np.mean(z)\n",
    "        return np.abs(mean_z)\n",
    "\n",
    "    consistency = generic_filter(grad_direction, direction_consistency, size=5)\n",
    "\n",
    "    # Cosmic string score\n",
    "    cosmic_string_score = consistency * normalize(grad_magnitude)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'edge_mean': np.mean(edges_norm),\n",
    "        'edge_std': np.std(edges_norm),\n",
    "        'edge_max': np.max(edges_norm),\n",
    "        'consistency_mean': np.mean(consistency),\n",
    "        'consistency_std': np.std(consistency),\n",
    "        'consistency_max': np.max(consistency),\n",
    "        'cosmic_string_score_mean': np.mean(cosmic_string_score),\n",
    "        'cosmic_string_score_std': np.std(cosmic_string_score),\n",
    "        'cosmic_string_score_max': np.max(cosmic_string_score),\n",
    "    }\n",
    "\n",
    "    return metrics, edges_norm, cosmic_string_score\n",
    "\n",
    "# Calculate metrics for the real CMB map\n",
    "print(\"Calculating metrics for the real CMB map...\")\n",
    "real_metrics, real_edges_norm, real_cosmic_string_score = calculate_feature_metrics(img_cleaned)\n",
    "\n",
    "# Calculate metrics for each random map\n",
    "print(\"Calculating metrics for random simulations...\")\n",
    "random_metrics_list = []\n",
    "random_edges_norm_list = []\n",
    "random_cosmic_string_score_list = []\n",
    "\n",
    "for i, random_map in enumerate(random_maps):\n",
    "    print(f\"Processing simulation {i+1}/{n_simulations}...\")\n",
    "    metrics, edges_norm, cosmic_string_score = calculate_feature_metrics(random_map)\n",
    "    random_metrics_list.append(metrics)\n",
    "    random_edges_norm_list.append(edges_norm)\n",
    "    random_cosmic_string_score_list.append(cosmic_string_score)\n",
    "\n",
    "# Compute statistics across random simulations\n",
    "random_stats = {}\n",
    "for key in real_metrics.keys():\n",
    "    values = [metrics[key] for metrics in random_metrics_list]\n",
    "    random_stats[key] = {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values)\n",
    "    }\n",
    "\n",
    "# Calculate significance (z-scores) for each metric\n",
    "significance = {}\n",
    "for key in real_metrics.keys():\n",
    "    mean = random_stats[key]['mean']\n",
    "    std = random_stats[key]['std']\n",
    "    z_score = (real_metrics[key] - mean) / std\n",
    "    significance[key] = {\n",
    "        'real_value': real_metrics[key],\n",
    "        'random_mean': mean,\n",
    "        'random_std': std,\n",
    "        'z_score': z_score,\n",
    "        'p_value': 2 * (1 - stats.norm.cdf(abs(z_score)))  # Two-tailed p-value\n",
    "    }\n",
    "\n",
    "# Display significance results\n",
    "print(\"\\nStatistical Significance of Feature Metrics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<25} {'Real Value':<15} {'Random Mean':<15} {'Random Std':<15} {'Z-Score':<10} {'P-Value':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for key, values in significance.items():\n",
    "    print(f\"{key:<25} {values['real_value']:<15.6f} {values['random_mean']:<15.6f} {values['random_std']:<15.6f} {values['z_score']:<10.2f} {values['p_value']:<10.6f}\")\n",
    "\n",
    "# Visualize the distribution of metrics across random simulations\n",
    "plt.figure(figsize=(15, 10))\n",
    "metrics_to_plot = ['edge_max', 'consistency_max', 'cosmic_string_score_max']\n",
    "for i, key in enumerate(metrics_to_plot):\n",
    "    plt.subplot(len(metrics_to_plot), 1, i+1)\n",
    "    values = [metrics[key] for metrics in random_metrics_list]\n",
    "    plt.hist(values, bins=20, alpha=0.7)\n",
    "    plt.axvline(real_metrics[key], color='red', linestyle='--', \n",
    "                label=f'Real CMB (z={significance[key][\"z_score\"]:.2f})')\n",
    "    plt.title(f'Distribution of {key} across Random Simulations')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify regions with statistically significant features\n",
    "# Calculate pixel-wise z-scores for cosmic string score\n",
    "random_cosmic_string_score_mean = np.mean(random_cosmic_string_score_list, axis=0)\n",
    "random_cosmic_string_score_std = np.std(random_cosmic_string_score_list, axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "random_cosmic_string_score_std = np.where(random_cosmic_string_score_std > 0, \n",
    "                                         random_cosmic_string_score_std, 1e-10)\n",
    "\n",
    "# Calculate z-score map\n",
    "z_score_map = (real_cosmic_string_score - random_cosmic_string_score_mean) / random_cosmic_string_score_std\n",
    "\n",
    "# Create significance mask (e.g., z > 3 is significant at p < 0.003)\n",
    "significance_threshold = 3.0\n",
    "significant_features_mask = z_score_map > significance_threshold\n",
    "\n",
    "# Visualize significant features\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(real_cosmic_string_score, cmap='inferno')\n",
    "plt.title('Cosmic String Score (Real CMB)')\n",
    "plt.colorbar(label='Score')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(random_cosmic_string_score_mean, cmap='inferno')\n",
    "plt.title('Mean Cosmic String Score (Random CMB)')\n",
    "plt.colorbar(label='Score')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(z_score_map, cmap='coolwarm', vmin=-5, vmax=5)\n",
    "plt.title('Z-Score Map')\n",
    "plt.colorbar(label='Z-Score')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(img_cleaned, cmap='inferno')\n",
    "plt.imshow(significant_features_mask, cmap='binary', alpha=0.3)\n",
    "plt.title(f'Statistically Significant Features (Z > {significance_threshold})')\n",
    "plt.colorbar(label='Is Significant')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count and analyze significant features\n",
    "num_significant_pixels = np.sum(significant_features_mask)\n",
    "total_pixels = significant_features_mask.size\n",
    "significant_percentage = (num_significant_pixels / total_pixels) * 100\n",
    "\n",
    "print(f\"\\nSignificant Feature Analysis (Z > {significance_threshold}):\")\n",
    "print(f\"  Number of significant pixels: {num_significant_pixels}\")\n",
    "print(f\"  Percentage of significant pixels: {significant_percentage:.4f}%\")\n",
    "print(f\"  Expected percentage by chance: {100 * (1 - stats.norm.cdf(significance_threshold)):.6f}%\")\n",
    "\n",
    "# If there are significant features, analyze their properties\n",
    "if num_significant_pixels > 0:\n",
    "    # Extract properties of significant features\n",
    "    significant_values = real_cosmic_string_score[significant_features_mask]\n",
    "\n",
    "    # Calculate statistics\n",
    "    print(\"\\nProperties of Significant Features:\")\n",
    "    print(f\"  Mean value: {np.mean(significant_values):.6f}\")\n",
    "    print(f\"  Median value: {np.median(significant_values):.6f}\")\n",
    "    print(f\"  Standard deviation: {np.std(significant_values):.6f}\")\n",
    "    print(f\"  Min value: {np.min(significant_values):.6f}\")\n",
    "    print(f\"  Max value: {np.max(significant_values):.6f}\")\n",
    "\n",
    "    # Visualize distribution of significant feature values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(significant_values, bins=30, alpha=0.7, color='purple')\n",
    "    plt.title('Distribution of Significant Feature Values')\n",
    "    plt.xlabel('Cosmic String Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ],
   "id": "2e95e80c46190af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Interactive Visualization of Results\n",
    "\n",
    "Interactive visualizations allow users to dynamically explore the data and gain deeper insights. In this section, we'll create interactive plots to better explore the CMB data and the results of our various analyses.\n"
   ],
   "id": "5d96008613567f10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries for interactive visualization\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "\n",
    "# Create a function for interactive visualization of different maps\n",
    "def interactive_map_viewer(maps_dict):\n",
    "    \"\"\"\n",
    "    Create an interactive viewer for multiple maps.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    maps_dict : dict\n",
    "        Dictionary of maps to visualize, with keys as map names and values as map arrays\n",
    "    \"\"\"\n",
    "    # Create dropdown for map selection\n",
    "    map_dropdown = widgets.Dropdown(\n",
    "        options=list(maps_dict.keys()),\n",
    "        value=list(maps_dict.keys())[0],\n",
    "        description='Map:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create colormap dropdown\n",
    "    colormap_dropdown = widgets.Dropdown(\n",
    "        options=['inferno', 'viridis', 'plasma', 'magma', 'cividis', 'coolwarm', 'RdBu_r', 'jet'],\n",
    "        value='inferno',\n",
    "        description='Colormap:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create sliders for min/max values\n",
    "    min_percentile = widgets.FloatSlider(\n",
    "        value=1.0,\n",
    "        min=0.0,\n",
    "        max=49.0,\n",
    "        step=1.0,\n",
    "        description='Min Percentile:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    max_percentile = widgets.FloatSlider(\n",
    "        value=99.0,\n",
    "        min=51.0,\n",
    "        max=100.0,\n",
    "        step=1.0,\n",
    "        description='Max Percentile:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create output widget for the plot\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Function to update the plot\n",
    "    def update_plot(*args):\n",
    "        with output:\n",
    "            output.clear_output(wait=True)\n",
    "\n",
    "            # Get selected map\n",
    "            selected_map = maps_dict[map_dropdown.value]\n",
    "\n",
    "            # Calculate percentile values for colormap limits\n",
    "            vmin = np.percentile(selected_map, min_percentile.value)\n",
    "            vmax = np.percentile(selected_map, max_percentile.value)\n",
    "\n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            im = ax.imshow(selected_map, cmap=colormap_dropdown.value, vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f'{map_dropdown.value}')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Connect the widgets to the update function\n",
    "    map_dropdown.observe(update_plot, names='value')\n",
    "    colormap_dropdown.observe(update_plot, names='value')\n",
    "    min_percentile.observe(update_plot, names='value')\n",
    "    max_percentile.observe(update_plot, names='value')\n",
    "\n",
    "    # Create the initial plot\n",
    "    update_plot()\n",
    "\n",
    "    # Create the UI layout\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([map_dropdown, colormap_dropdown]),\n",
    "        widgets.HBox([min_percentile, max_percentile]),\n",
    "        output\n",
    "    ])\n",
    "\n",
    "    return ui\n",
    "\n",
    "# Create a dictionary of maps to visualize\n",
    "maps_to_visualize = {\n",
    "    'Original CMB': img_cleaned,\n",
    "    'Edge Detection': edges_norm,\n",
    "    'Anomaly Detection': anomaly_norm,\n",
    "    'Wavelet-Enhanced': enhanced_img,\n",
    "    'Wavelet-Based Edge Detection': edges_denoised_norm,\n",
    "    'Cosmic String Score': cosmic_string_score,\n",
    "    'Z-Score Map': z_score_map,\n",
    "    'Significant Features': significant_features_mask.astype(float)\n",
    "}\n",
    "\n",
    "# Display the interactive map viewer\n",
    "print(\"Interactive Map Viewer - Select a map and adjust visualization parameters:\")\n",
    "display(interactive_map_viewer(maps_to_visualize))\n"
   ],
   "id": "734fe77940162c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a function for interactive comparison of two maps\n",
    "def interactive_map_comparison(maps_dict):\n",
    "    \"\"\"\n",
    "    Create an interactive viewer to compare two maps side by side or as an overlay.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    maps_dict : dict\n",
    "        Dictionary of maps to visualize, with keys as map names and values as map arrays\n",
    "    \"\"\"\n",
    "    # Create dropdowns for map selection\n",
    "    map1_dropdown = widgets.Dropdown(\n",
    "        options=list(maps_dict.keys()),\n",
    "        value=list(maps_dict.keys())[0],\n",
    "        description='Map 1:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    map2_dropdown = widgets.Dropdown(\n",
    "        options=list(maps_dict.keys()),\n",
    "        value=list(maps_dict.keys())[1] if len(maps_dict) > 1 else list(maps_dict.keys())[0],\n",
    "        description='Map 2:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create colormap dropdowns\n",
    "    cmap1_dropdown = widgets.Dropdown(\n",
    "        options=['inferno', 'viridis', 'plasma', 'magma', 'cividis', 'coolwarm', 'RdBu_r', 'jet'],\n",
    "        value='inferno',\n",
    "        description='Colormap 1:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    cmap2_dropdown = widgets.Dropdown(\n",
    "        options=['inferno', 'viridis', 'plasma', 'magma', 'cividis', 'coolwarm', 'RdBu_r', 'jet'],\n",
    "        value='coolwarm',\n",
    "        description='Colormap 2:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create radio buttons for display mode\n",
    "    display_mode = widgets.RadioButtons(\n",
    "        options=['Side by Side', 'Overlay'],\n",
    "        value='Side by Side',\n",
    "        description='Display Mode:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create slider for overlay transparency\n",
    "    alpha_slider = widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.05,\n",
    "        description='Overlay Transparency:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        disabled=display_mode.value != 'Overlay'\n",
    "    )\n",
    "\n",
    "    # Create output widget for the plot\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Function to update the plot\n",
    "    def update_plot(*args):\n",
    "        with output:\n",
    "            output.clear_output(wait=True)\n",
    "\n",
    "            # Get selected maps\n",
    "            map1 = maps_dict[map1_dropdown.value]\n",
    "            map2 = maps_dict[map2_dropdown.value]\n",
    "\n",
    "            # Create figure\n",
    "            if display_mode.value == 'Side by Side':\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "                # Plot map 1\n",
    "                im1 = ax1.imshow(map1, cmap=cmap1_dropdown.value)\n",
    "                ax1.set_title(map1_dropdown.value)\n",
    "                plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "                # Plot map 2\n",
    "                im2 = ax2.imshow(map2, cmap=cmap2_dropdown.value)\n",
    "                ax2.set_title(map2_dropdown.value)\n",
    "                plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "            else:  # Overlay mode\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "                # Plot map 1 as base\n",
    "                im1 = ax.imshow(map1, cmap=cmap1_dropdown.value)\n",
    "\n",
    "                # Overlay map 2 with transparency\n",
    "                im2 = ax.imshow(map2, cmap=cmap2_dropdown.value, alpha=alpha_slider.value)\n",
    "\n",
    "                ax.set_title(f'Overlay: {map1_dropdown.value} + {map2_dropdown.value}')\n",
    "\n",
    "                # Add colorbars\n",
    "                from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "                divider = make_axes_locatable(ax)\n",
    "\n",
    "                cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                cbar1 = plt.colorbar(im1, cax=cax1)\n",
    "                cbar1.set_label(map1_dropdown.value)\n",
    "\n",
    "                cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.3)\n",
    "                cbar2 = plt.colorbar(im2, cax=cax2)\n",
    "                cbar2.set_label(map2_dropdown.value)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Function to update alpha slider state\n",
    "    def update_alpha_state(*args):\n",
    "        alpha_slider.disabled = display_mode.value != 'Overlay'\n",
    "\n",
    "    # Connect the widgets to the update functions\n",
    "    map1_dropdown.observe(update_plot, names='value')\n",
    "    map2_dropdown.observe(update_plot, names='value')\n",
    "    cmap1_dropdown.observe(update_plot, names='value')\n",
    "    cmap2_dropdown.observe(update_plot, names='value')\n",
    "    display_mode.observe(update_plot, names='value')\n",
    "    display_mode.observe(update_alpha_state, names='value')\n",
    "    alpha_slider.observe(update_plot, names='value')\n",
    "\n",
    "    # Create the initial plot\n",
    "    update_plot()\n",
    "\n",
    "    # Create the UI layout\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([map1_dropdown, map2_dropdown]),\n",
    "        widgets.HBox([cmap1_dropdown, cmap2_dropdown]),\n",
    "        widgets.HBox([display_mode, alpha_slider]),\n",
    "        output\n",
    "    ])\n",
    "\n",
    "    return ui\n",
    "\n",
    "# Display the interactive map comparison\n",
    "print(\"\\nInteractive Map Comparison - Compare two maps side by side or as an overlay:\")\n",
    "display(interactive_map_comparison(maps_to_visualize))\n"
   ],
   "id": "b0245bfbb5f30535",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a function for interactive region exploration\n",
    "def interactive_region_explorer(base_map, feature_maps):\n",
    "    \"\"\"\n",
    "    Create an interactive viewer to explore specific regions of interest.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_map : array\n",
    "        The base map to display\n",
    "    feature_maps : dict\n",
    "        Dictionary of feature maps to overlay, with keys as map names and values as map arrays\n",
    "    \"\"\"\n",
    "    # Create sliders for region selection\n",
    "    x_center = widgets.IntSlider(\n",
    "        value=base_map.shape[1] // 2,\n",
    "        min=0,\n",
    "        max=base_map.shape[1] - 1,\n",
    "        step=1,\n",
    "        description='X Center:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    y_center = widgets.IntSlider(\n",
    "        value=base_map.shape[0] // 2,\n",
    "        min=0,\n",
    "        max=base_map.shape[0] - 1,\n",
    "        step=1,\n",
    "        description='Y Center:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    window_size = widgets.IntSlider(\n",
    "        value=100,\n",
    "        min=20,\n",
    "        max=500,\n",
    "        step=10,\n",
    "        description='Window Size:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create dropdown for feature map selection\n",
    "    feature_dropdown = widgets.Dropdown(\n",
    "        options=list(feature_maps.keys()),\n",
    "        value=list(feature_maps.keys())[0],\n",
    "        description='Feature Map:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create slider for overlay transparency\n",
    "    alpha_slider = widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.05,\n",
    "        description='Overlay Transparency:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    # Create output widget for the plot\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Function to update the plot\n",
    "    def update_plot(*args):\n",
    "        with output:\n",
    "            output.clear_output(wait=True)\n",
    "\n",
    "            # Calculate region boundaries\n",
    "            half_window = window_size.value // 2\n",
    "            x_min = max(0, x_center.value - half_window)\n",
    "            x_max = min(base_map.shape[1], x_center.value + half_window)\n",
    "            y_min = max(0, y_center.value - half_window)\n",
    "            y_max = min(base_map.shape[0], y_center.value + half_window)\n",
    "\n",
    "            # Extract regions from maps\n",
    "            base_region = base_map[y_min:y_max, x_min:x_max]\n",
    "            feature_region = feature_maps[feature_dropdown.value][y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "            # Plot base map\n",
    "            im1 = ax.imshow(base_region, cmap='inferno')\n",
    "\n",
    "            # Overlay feature map\n",
    "            im2 = ax.imshow(feature_region, cmap='coolwarm', alpha=alpha_slider.value)\n",
    "\n",
    "            ax.set_title(f'Region ({x_min}:{x_max}, {y_min}:{y_max}) - {feature_dropdown.value} Overlay')\n",
    "\n",
    "            # Add colorbars\n",
    "            from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "            divider = make_axes_locatable(ax)\n",
    "\n",
    "            cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cbar1 = plt.colorbar(im1, cax=cax1)\n",
    "            cbar1.set_label('Base Map')\n",
    "\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.3)\n",
    "            cbar2 = plt.colorbar(im2, cax=cax2)\n",
    "            cbar2.set_label(feature_dropdown.value)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Print region statistics\n",
    "            print(f\"Region Statistics for {feature_dropdown.value}:\")\n",
    "            print(f\"  Mean: {np.mean(feature_region):.6f}\")\n",
    "            print(f\"  Median: {np.median(feature_region):.6f}\")\n",
    "            print(f\"  Standard Deviation: {np.std(feature_region):.6f}\")\n",
    "            print(f\"  Min: {np.min(feature_region):.6f}\")\n",
    "            print(f\"  Max: {np.max(feature_region):.6f}\")\n",
    "\n",
    "    # Connect the widgets to the update function\n",
    "    x_center.observe(update_plot, names='value')\n",
    "    y_center.observe(update_plot, names='value')\n",
    "    window_size.observe(update_plot, names='value')\n",
    "    feature_dropdown.observe(update_plot, names='value')\n",
    "    alpha_slider.observe(update_plot, names='value')\n",
    "\n",
    "    # Create the initial plot\n",
    "    update_plot()\n",
    "\n",
    "    # Create the UI layout\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([x_center, y_center]),\n",
    "        widgets.HBox([window_size, feature_dropdown]),\n",
    "        widgets.HBox([alpha_slider]),\n",
    "        output\n",
    "    ])\n",
    "\n",
    "    return ui\n",
    "\n",
    "# Display the interactive region explorer\n",
    "print(\"\\nInteractive Region Explorer - Zoom in on specific regions of interest:\")\n",
    "display(interactive_region_explorer(img_cleaned, maps_to_visualize))\n"
   ],
   "id": "3ad94bace8eef1d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Large-Scale Structure Analysis in the CMB\n",
    "\n",
    "While previous sections focused on detecting small-scale features like cosmic strings, this section explores large-scale structures in the CMB. Large-scale structures correspond to low multipoles (ℓ < 30) and can provide insights into the early universe, inflation, and potential anomalies.\n"
   ],
   "id": "aabba1db900e2a95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import scipy.stats as stats\n",
    "from scipy.special import legendre\n",
    "\n",
    "# Ensure we have the CMB map loaded\n",
    "if 'cmb_map' not in locals():\n",
    "    cmb_map = hp.read_map('data/COM_CMB_IQU-smica_2048_R3.00_full.fits', field=0)\n",
    "    nside = hp.get_nside(cmb_map)\n",
    "    npix = hp.nside2npix(nside)\n",
    "\n",
    "# Calculate power spectrum\n",
    "cl = hp.anafast(cmb_map, lmax=100)\n",
    "ell = np.arange(len(cl))\n",
    "\n",
    "# Focus on large scales (low multipoles)\n",
    "lmax_large = 30  # Define large scales as ℓ < 30\n",
    "\n",
    "# Plot the power spectrum for large scales\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ell[1:lmax_large+1], cl[1:lmax_large+1], 'o-', color='darkblue', lw=2)\n",
    "plt.title('CMB Power Spectrum at Large Scales (Low Multipoles)')\n",
    "plt.xlabel('Multipole ℓ')\n",
    "plt.ylabel('C$_\\\\ell$ [μK$^2$]')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the theoretical ΛCDM power spectrum for comparison\n",
    "# This is a simplified model - in practice, you would use CAMB or CLASS\n",
    "def simple_lambda_cdm(ell, A_s=2.1e-9, n_s=0.965):\n",
    "    \"\"\"Simple power-law primordial power spectrum with ΛCDM transfer function.\"\"\"\n",
    "    # This is a very simplified approximation\n",
    "    ell = np.asarray(ell)\n",
    "    ell[ell == 0] = 1  # Avoid division by zero\n",
    "\n",
    "    # Power-law spectrum with Sachs-Wolfe effect\n",
    "    cl_theory = A_s * (ell / 80.0) ** (n_s - 1.0) * 2 * np.pi / (ell * (ell + 1))\n",
    "\n",
    "    # Scale to match observed spectrum approximately\n",
    "    cl_theory *= 1e10\n",
    "    return cl_theory\n",
    "\n",
    "# Calculate theoretical spectrum\n",
    "cl_theory = simple_lambda_cdm(ell[1:lmax_large+1])\n",
    "\n",
    "# Plot observed vs theoretical\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ell[1:lmax_large+1], cl[1:lmax_large+1], 'o-', color='darkblue', lw=2, label='Observed')\n",
    "plt.plot(ell[1:lmax_large+1], cl_theory, '--', color='red', lw=2, label='ΛCDM (simplified)')\n",
    "plt.title('CMB Power Spectrum: Observed vs ΛCDM Model')\n",
    "plt.xlabel('Multipole ℓ')\n",
    "plt.ylabel('C$_\\\\ell$ [μK$^2$]')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = cl[1:lmax_large+1] - cl_theory\n",
    "percent_diff = 100 * residuals / cl_theory\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(ell[1:lmax_large+1], percent_diff, color='purple', alpha=0.7)\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.title('Residuals: Observed - ΛCDM Model')\n",
    "plt.xlabel('Multipole ℓ')\n",
    "plt.ylabel('Difference (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "af70482554f501fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.1 Spherical Harmonic Analysis of Large-Scale Structures\n",
    "\n",
    "Spherical harmonics provide a natural basis for analyzing the CMB, as they capture the angular patterns on the sky. Low-order spherical harmonics correspond to large-scale structures.\n"
   ],
   "id": "53e5b58e43668d9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the low-order spherical harmonics maps\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot the first few multipoles (ℓ = 1, 2, 3, 4)\n",
    "for i, l in enumerate([1, 2, 3, 4]):\n",
    "    # Get the spherical harmonic coefficients for this multipole\n",
    "    alm = hp.map2alm(cmb_map, lmax=l)\n",
    "\n",
    "    # Set all coefficients to zero except for the current multipole\n",
    "    alm_filtered = np.zeros_like(alm, dtype=complex)\n",
    "\n",
    "    # Find indices corresponding to the current multipole\n",
    "    idx = hp.Alm.getidx(l, l, np.arange(0, l+1))\n",
    "    alm_filtered[idx] = alm[idx]\n",
    "\n",
    "    # Convert back to map\n",
    "    map_filtered = hp.alm2map(alm_filtered, nside)\n",
    "\n",
    "    # Plot\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    hp.mollview(map_filtered, title=f'ℓ = {l} (Large-Scale Structure)', hold=True, cmap='coolwarm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the power in each multipole\n",
    "power_per_l = np.zeros(lmax_large+1)\n",
    "for l in range(1, lmax_large+1):\n",
    "    # Get the spherical harmonic coefficients for this multipole\n",
    "    alm = hp.map2alm(cmb_map, lmax=l)\n",
    "\n",
    "    # Set all coefficients to zero except for the current multipole\n",
    "    alm_filtered = np.zeros_like(alm, dtype=complex)\n",
    "\n",
    "    # Find indices corresponding to the current multipole\n",
    "    idx = hp.Alm.getidx(l, l, np.arange(0, l+1))\n",
    "    alm_filtered[idx] = alm[idx]\n",
    "\n",
    "    # Convert back to map\n",
    "    map_filtered = hp.alm2map(alm_filtered, nside)\n",
    "\n",
    "    # Calculate power (variance of the map)\n",
    "    power_per_l[l] = np.var(map_filtered)\n",
    "\n",
    "# Plot power per multipole\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, lmax_large+1), power_per_l[1:], color='teal', alpha=0.7)\n",
    "plt.title('Power in Each Multipole (Large-Scale Structures)')\n",
    "plt.xlabel('Multipole ℓ')\n",
    "plt.ylabel('Power (Variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "84b2a0314aa7d9e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.2 Angular Correlation Function Analysis\n",
    "\n",
    "The angular correlation function measures how temperature fluctuations are correlated at different angular separations. It's particularly useful for studying large-scale correlations in the CMB.\n"
   ],
   "id": "6d8ba13e909a04bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the angular correlation function\n",
    "def angular_correlation(map_data, nside, n_theta=100):\n",
    "    \"\"\"Calculate the angular correlation function C(θ).\"\"\"\n",
    "    # Generate random points on the sphere\n",
    "    npix = hp.nside2npix(nside)\n",
    "    n_points = 10000  # Number of random points to use\n",
    "\n",
    "    # Generate random pixel indices\n",
    "    pixels = np.random.choice(npix, size=n_points, replace=False)\n",
    "\n",
    "    # Get the values at these pixels\n",
    "    values = map_data[pixels]\n",
    "\n",
    "    # Get the coordinates of these pixels\n",
    "    theta, phi = hp.pix2ang(nside, pixels)\n",
    "\n",
    "    # Convert to 3D vectors\n",
    "    x = np.sin(theta) * np.cos(phi)\n",
    "    y = np.sin(theta) * np.sin(phi)\n",
    "    z = np.cos(theta)\n",
    "    vectors = np.column_stack((x, y, z))\n",
    "\n",
    "    # Calculate all pairwise dot products\n",
    "    dot_products = np.dot(vectors, vectors.T)\n",
    "\n",
    "    # Clip to valid range for arccos\n",
    "    dot_products = np.clip(dot_products, -1.0, 1.0)\n",
    "\n",
    "    # Convert to angles in degrees\n",
    "    angles = np.arccos(dot_products) * 180.0 / np.pi\n",
    "\n",
    "    # Calculate all pairwise products of values\n",
    "    value_products = np.outer(values, values)\n",
    "\n",
    "    # Create angle bins\n",
    "    theta_bins = np.linspace(0, 180, n_theta+1)\n",
    "    theta_centers = 0.5 * (theta_bins[1:] + theta_bins[:-1])\n",
    "\n",
    "    # Calculate correlation function\n",
    "    correlation = np.zeros(n_theta)\n",
    "    counts = np.zeros(n_theta)\n",
    "\n",
    "    # Bin the angles and calculate mean product in each bin\n",
    "    for i in range(n_points):\n",
    "        for j in range(i+1, n_points):\n",
    "            angle = angles[i, j]\n",
    "            bin_idx = np.digitize(angle, theta_bins) - 1\n",
    "            if 0 <= bin_idx < n_theta:\n",
    "                correlation[bin_idx] += value_products[i, j]\n",
    "                counts[bin_idx] += 1\n",
    "\n",
    "    # Normalize by counts\n",
    "    mask = counts > 0\n",
    "    correlation[mask] /= counts[mask]\n",
    "\n",
    "    return theta_centers, correlation\n",
    "\n",
    "# Calculate the angular correlation function\n",
    "theta, corr = angular_correlation(cmb_map, nside, n_theta=50)\n",
    "\n",
    "# Plot the angular correlation function\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(theta, corr, 'o-', color='darkblue', lw=2)\n",
    "plt.title('CMB Angular Correlation Function')\n",
    "plt.xlabel('Angular Separation θ (degrees)')\n",
    "plt.ylabel('C(θ) [μK$^2$]')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the theoretical correlation function\n",
    "# This is a simplified model using Legendre polynomials\n",
    "def theoretical_correlation(theta_deg, cl):\n",
    "    \"\"\"Calculate theoretical correlation function from power spectrum.\"\"\"\n",
    "    # Convert to radians\n",
    "    theta_rad = theta_deg * np.pi / 180.0\n",
    "\n",
    "    # Initialize correlation function\n",
    "    corr_theory = np.zeros_like(theta_rad)\n",
    "\n",
    "    # Sum over multipoles\n",
    "    lmax = len(cl) - 1\n",
    "    for l in range(1, lmax+1):\n",
    "        # Legendre polynomial\n",
    "        leg = legendre(l)(np.cos(theta_rad))\n",
    "\n",
    "        # Add contribution from this multipole\n",
    "        corr_theory += (2*l + 1) * cl[l] * leg / (4 * np.pi)\n",
    "\n",
    "    return corr_theory\n",
    "\n",
    "# Calculate theoretical correlation function\n",
    "corr_theory = theoretical_correlation(theta, cl)\n",
    "\n",
    "# Plot observed vs theoretical\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(theta, corr, 'o-', color='darkblue', lw=2, label='Observed')\n",
    "plt.plot(theta, corr_theory, '--', color='red', lw=2, label='Theory')\n",
    "plt.title('CMB Angular Correlation Function: Observed vs Theory')\n",
    "plt.xlabel('Angular Separation θ (degrees)')\n",
    "plt.ylabel('C(θ) [μK$^2$]')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "7c5d2f3df80c576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.3 Topological Analysis of Large-Scale Structures\n",
    "\n",
    "Topology provides a way to characterize the morphology of CMB temperature fluctuations. Minkowski functionals are particularly useful for quantifying the topology of the CMB and can reveal non-Gaussian features.\n"
   ],
   "id": "66a2a5443868423d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Implement a simplified version of Minkowski functionals\n",
    "def minkowski_functionals(map_data, thresholds):\n",
    "    \"\"\"\n",
    "    Calculate simplified Minkowski functionals for a 2D map.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    map_data : 2D array\n",
    "        The map data\n",
    "    thresholds : array\n",
    "        Threshold values to calculate functionals at\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    v0, v1, v2 : arrays\n",
    "        The three Minkowski functionals at each threshold\n",
    "    \"\"\"\n",
    "    # Initialize functionals\n",
    "    n_thresh = len(thresholds)\n",
    "    v0 = np.zeros(n_thresh)  # Area\n",
    "    v1 = np.zeros(n_thresh)  # Perimeter\n",
    "    v2 = np.zeros(n_thresh)  # Euler characteristic\n",
    "\n",
    "    # Calculate functionals at each threshold\n",
    "    for i, thresh in enumerate(thresholds):\n",
    "        # Create binary map\n",
    "        binary_map = (map_data > thresh).astype(int)\n",
    "\n",
    "        # V0: Area (fraction of pixels above threshold)\n",
    "        v0[i] = np.mean(binary_map)\n",
    "\n",
    "        # V1: Perimeter (simplified using edge detection)\n",
    "        edges = ndimage.sobel(binary_map)\n",
    "        v1[i] = np.sum(edges > 0) / binary_map.size\n",
    "\n",
    "        # V2: Euler characteristic (simplified using structure count)\n",
    "        # Label connected structures\n",
    "        labeled, num_features = ndimage.label(binary_map)\n",
    "\n",
    "        # Count holes using inverted map\n",
    "        labeled_inv, num_holes = ndimage.label(1 - binary_map)\n",
    "\n",
    "        # Euler characteristic = structures - holes\n",
    "        v2[i] = (num_features - num_holes) / binary_map.size\n",
    "\n",
    "    return v0, v1, v2\n",
    "\n",
    "# Convert HEALPix map to 2D for topological analysis\n",
    "if 'img_cleaned' not in locals():\n",
    "    img = hp.mollview(cmb_map, return_projected_map=True, nest=False, title='CMB Temperature Map', cmap='inferno', xsize=2000, hold=True)\n",
    "    img_cleaned = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Define thresholds based on standard deviations\n",
    "sigma = np.std(img_cleaned[~np.isnan(img_cleaned)])\n",
    "mean = np.mean(img_cleaned[~np.isnan(img_cleaned)])\n",
    "thresholds = np.linspace(mean - 3*sigma, mean + 3*sigma, 25)\n",
    "\n",
    "# Calculate Minkowski functionals\n",
    "v0, v1, v2 = minkowski_functionals(img_cleaned, thresholds)\n",
    "\n",
    "# Plot Minkowski functionals\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(thresholds, v0, 'o-', color='darkblue', lw=2)\n",
    "plt.title('V0: Area Fraction')\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(thresholds, v1, 'o-', color='darkred', lw=2)\n",
    "plt.title('V1: Perimeter')\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(thresholds, v2, 'o-', color='darkgreen', lw=2)\n",
    "plt.title('V2: Euler Characteristic')\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate Gaussian random field for comparison\n",
    "np.random.seed(42)\n",
    "gaussian_map = np.random.normal(mean, sigma, img_cleaned.shape)\n",
    "\n",
    "# Calculate Minkowski functionals for Gaussian field\n",
    "v0_gauss, v1_gauss, v2_gauss = minkowski_functionals(gaussian_map, thresholds)\n",
    "\n",
    "# Plot comparison with Gaussian random field\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(thresholds, v0, 'o-', color='darkblue', lw=2, label='CMB')\n",
    "plt.plot(thresholds, v0_gauss, '--', color='gray', lw=2, label='Gaussian')\n",
    "plt.title('V0: Area Fraction')\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(thresholds, v1, 'o-', color='darkred', lw=2, label='CMB')\n",
    "plt.plot(thresholds, v1_gauss, '--', color='gray', lw=2, label='Gaussian')\n",
    "plt.title('V1: Perimeter')\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(thresholds, v2, 'o-', color='darkgreen', lw=2, label='CMB')\n",
    "plt.plot(thresholds, v2_gauss, '--', color='gray', lw=2, label='Gaussian')\n",
    "plt.title('V2: Euler Characteristic')\n",
    "plt.xlabel('Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a2706b8bfeeaaca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10.4 Machine Learning for Large-Scale Structure Detection\n",
    "\n",
    "Machine learning can help identify patterns and anomalies in large-scale CMB structures that might be difficult to detect with traditional methods.\n"
   ],
   "id": "4b206c19a2f527fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract large-scale features from the CMB map\n",
    "def extract_large_scale_features(cmb_map, lmax=30):\n",
    "    \"\"\"Extract large-scale features from CMB map using spherical harmonics.\"\"\"\n",
    "    # Get spherical harmonic coefficients\n",
    "    alm = hp.map2alm(cmb_map, lmax=lmax)\n",
    "\n",
    "    # Convert to power spectrum\n",
    "    cl = hp.alm2cl(alm)\n",
    "\n",
    "    # Get real and imaginary parts of alm coefficients\n",
    "    alm_real = np.real(alm)\n",
    "    alm_imag = np.imag(alm)\n",
    "\n",
    "    # Combine into feature vector\n",
    "    features = np.concatenate([cl, alm_real, alm_imag])\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "lmax_large = 30\n",
    "large_scale_features = extract_large_scale_features(cmb_map, lmax=lmax_large)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(large_scale_features.reshape(1, -1))\n",
    "\n",
    "# Generate a set of random CMB maps for comparison\n",
    "n_simulations = 10000\n",
    "print(f\"Generating {n_simulations} random CMB simulations for large-scale analysis...\")\n",
    "\n",
    "# Function to generate random CMB maps\n",
    "def generate_random_cmb(nside=2048, lmax=30, seed=None):\n",
    "    \"\"\"Generate a random Gaussian CMB map with the same power spectrum as the observed CMB.\"\"\"\n",
    "    # Set random seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Get power spectrum from the real CMB map\n",
    "    cl = hp.anafast(cmb_map, lmax=lmax)\n",
    "\n",
    "    # Generate random map with the same power spectrum\n",
    "    random_map = hp.synfast(cl, nside, lmax=lmax4)\n",
    "    print(\"Random CMB map generated.\")\n",
    "\n",
    "    return random_map\n",
    "\n",
    "# Generate random maps and extract features\n",
    "random_features = []\n",
    "for i in range(n_simulations):\n",
    "    print(f\"Generating random CMB map {i+1}/{n_simulations}...\")\n",
    "    random_map = generate_random_cmb(nside=nside, lmax=lmax_large, seed=i)\n",
    "    features = extract_large_scale_features(random_map, lmax=lmax_large)\n",
    "    random_features.append(features)\n",
    "\n",
    "# Convert to array and standardize\n",
    "random_features = np.array(random_features)\n",
    "random_features_scaled = scaler.transform(random_features)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "random_features_pca = pca.fit_transform(random_features_scaled)\n",
    "real_feature_pca = pca.transform(features_scaled)\n",
    "\n",
    "# Plot the first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(random_features_pca[:, 0], random_features_pca[:, 1], alpha=0.7, label='Random Simulations')\n",
    "plt.scatter(real_feature_pca[0, 0], real_feature_pca[0, 1], color='red', s=100, marker='*', label='Observed CMB')\n",
    "plt.title('PCA of Large-Scale CMB Features')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply Isolation Forest for anomaly detection\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso.fit(random_features_pca)\n",
    "\n",
    "# Calculate anomaly score for real CMB\n",
    "anomaly_score = -iso.score_samples(real_feature_pca)[0]\n",
    "print(f\"Anomaly score for observed CMB: {anomaly_score:.4f}\")\n",
    "\n",
    "# Calculate anomaly scores for random simulations\n",
    "random_scores = -iso.score_samples(random_features_pca)\n",
    "\n",
    "# Plot histogram of anomaly scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(random_scores, bins=20, alpha=0.7, color='blue', label='Random Simulations')\n",
    "plt.axvline(anomaly_score, color='red', linestyle='--', linewidth=2, label='Observed CMB')\n",
    "plt.title('Anomaly Scores for Large-Scale CMB Features')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = np.mean(random_scores >= anomaly_score)\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The observed CMB shows significant anomalies in large-scale structure compared to random simulations.\")\n",
    "else:\n",
    "    print(\"The observed CMB is consistent with random simulations at large scales.\")\n"
   ],
   "id": "cffb58763bc422bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Conclusion\n",
    "\n",
    "We've applied several machine learning and signal processing techniques to analyze the CMB data:\n",
    "\n",
    "1. **Feature Extraction**: Extracted patches from the CMB map and reduced dimensionality with PCA\n",
    "2. **Clustering**: Identified distinct patterns in the CMB data using K-means\n",
    "3. **Anomaly Detection**: Used Isolation Forest to find unusual patterns that might correspond to cosmic strings\n",
    "4. **Feature Importance**: Analyzed which principal components are most significant\n",
    "5. **Correlation Analysis**: Compared ML-based anomaly detection with traditional edge detection\n",
    "6. **Wavelet Analysis**: Applied wavelet transforms to detect multi-scale features and enhance cosmic string detection\n",
    "7. **Directional Analysis**: Analyzed the orientation and direction of detected edges to identify patterns characteristic of cosmic strings\n",
    "8. **Statistical Significance Testing**: Compared detected features against random simulations to assess their significance\n",
    "9. **Interactive Visualization**: Created interactive tools to explore the data and results dynamically\n",
    "10. **Large-Scale Structure Analysis**: Analyzed the CMB at large angular scales using power spectrum analysis, spherical harmonics, angular correlation functions, topological analysis, and machine learning\n",
    "\n",
    "These techniques provide complementary views of the CMB data and can help identify potential cosmic string candidates or other interesting features that might not be apparent through traditional analysis methods.\n",
    "\n",
    "The correlation between edge detection and anomaly detection suggests that ML methods can identify similar structures to traditional methods, but may also reveal additional patterns not captured by edge detection alone.\n",
    "\n",
    "Wavelet analysis provides a powerful tool for multi-scale feature detection, allowing us to enhance potential cosmic string signatures at specific scales while suppressing noise at other scales.\n",
    "\n",
    "Directional analysis helps identify regions with consistent gradient directions, which is a characteristic property of cosmic strings. By combining direction consistency with gradient magnitude, we can create a cosmic string score that highlights potential cosmic string candidates.\n",
    "\n",
    "Statistical significance testing allows us to distinguish genuine cosmic string candidates from statistical flukes by comparing our detected features against what would be expected from random Gaussian fluctuations in the CMB. This provides a rigorous framework for assessing the reality of potential cosmic string detections.\n",
    "\n",
    "Interactive visualization tools enable researchers to dynamically explore the data and results, facilitating deeper insights and more effective communication of findings. These tools allow for real-time adjustment of visualization parameters, comparison of different analysis methods, and detailed examination of regions of interest.\n",
    "\n",
    "Our large-scale structure analysis revealed patterns in the CMB at large angular scales, which correspond to the earliest observable epochs of the universe. The power spectrum analysis, spherical harmonic decomposition, angular correlation function, and topological analysis all provide complementary views of these large-scale structures. Machine learning techniques helped identify potential anomalies in these large-scale patterns compared to what would be expected from standard cosmological models.\n"
   ],
   "id": "b6dcc2d81ee3bc7d"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
