{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Exploration and Visualization of Cosmic Strings in CMB\n",
    "This notebook explores the Cosmic Microwave Background (CMB) data to identify potential cosmic string candidates."
   ],
   "id": "623af29cb778d5b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initial exploration of the CMB data using Healpy and Matplotlib.",
   "id": "398cb6ea7138181e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "# pip freeze > requirements.txt\n",
    "!pip install -r requirements.txt"
   ],
   "id": "69c5c13035d2a730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First exploring the COM_CMB_IQU-smica_2048_R3.00_full.fits file to understand its structure and contents.",
   "id": "8505d0be0a0656f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.constants.codata2014 import alpha\n",
    "from scipy.constants import h, c, k\n",
    "import healpy as hp\n",
    "\n",
    "# Planck function (spectral radiance)\n",
    "def planck(f, T):\n",
    "    x = h * f / (k * T)\n",
    "    return (2 * h * f**3 / c**2) / np.expm1(x)\n",
    "\n",
    "\n",
    "# Frequency range (Hz)\n",
    "frequencies = np.linspace(1e10, 1e12, 1000)  # 10 GHz to 1000 GHz\n",
    "temperature = 2.725  # CMB temperature in Kelvin\n",
    "\n",
    "# Compute spectral radiance\n",
    "radiance = planck(frequencies, temperature)\n",
    "\n",
    "# Plot spectrum\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, radiance, color='darkblue', label=f'T = {temperature} K')\n",
    "plt.title('CMB Blackbody Spectrum at T = 2.725 K')\n",
    "plt.xlabel('Frequency (GHz)')\n",
    "plt.ylabel('Spectral Radiance B(ν) [W·sr⁻¹·m⁻²·Hz⁻¹]')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4c33cfad7c5af566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "# Load a Planck CMB temperature map\n",
    "cmb_map = hp.read_map('data/COM_CMB_IQU-smica_2048_R3.00_full.fits', field=0)\n",
    "\n",
    "# Not ideal for CMB structure detection\n",
    "# cmb_map = hp.read_map('data/HFI_SkyMap_100_2048_R3.01_full.fits', field=0)\n",
    "\n",
    "\n",
    "# Convert to 2D image\n",
    "nside = hp.get_nside(cmb_map)\n",
    "npix = hp.nside2npix(nside)\n",
    "# Convert the CMB map to a 2D image for visualization\n",
    "img = hp.mollview(cmb_map, return_projected_map=True, nest=False, title='CMB Temperature Map', cmap='inferno', xsize=2000, hold=True)\n"
   ],
   "id": "f4d3f330ff83f3fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply edge detection (e.g., Sobel)edges = ndimage.sobel(img)\n",
    "# Plot the edges detected in the CMB map\n",
    "edges = ndimage.sobel(img)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = Path('./edge')\n",
    "\n",
    "if not folder_path.exists():\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Folder '{folder_path}' created successfully!\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "# Save to .fits\n",
    "from astropy.io import fits\n",
    "# Save as 2D FITS image\n",
    "hdu = fits.PrimaryHDU(edges.astype(np.float32))\n",
    "hdu.writeto(\"edge/edge_detected_image.fits\", overwrite=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(edges, cmap='inferno')\n",
    "plt.title('Possible Cosmic String Candidates (Edges in CMB)')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "2dfa72a8288c9645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Load or define your image (already done)\n",
    "# img = ...\n",
    "\n",
    "# --- Step 2: Force-clean the CMB image ---\n",
    "# Replace NaNs, -inf, +inf with zeros (or median)\n",
    "img_cleaned = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# --- Step 3: Edge detection ---\n",
    "edges = ndimage.sobel(img_cleaned)\n",
    "\n",
    "# --- Step 4: Print CLEANED ranges ---\n",
    "print(\"Cleaned CMB image range:\", np.min(img_cleaned), \"to\", np.max(img_cleaned))\n",
    "print(\"Edge image range:\", np.min(edges), \"to\", np.max(edges))\n",
    "\n",
    "# --- Step 5: Normalize for plotting ---\n",
    "def normalize(arr):\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return (arr - np.min(arr)) / (np.ptp(arr) + 1e-8)\n",
    "\n",
    "img_norm = normalize(img_cleaned)\n",
    "edges_norm = normalize(edges)\n",
    "\n",
    "# Optional: Threshold edges for clarity\n",
    "edges_thresh = np.where(edges_norm > 0.3, 1.0, 0.0)\n",
    "\n",
    "# --- Step 6: Plot overlay ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(img_norm, cmap='coolwarm',alpha=0.9, origin='lower', aspect='auto')\n",
    "plt.imshow(edges_thresh, cmap='YlOrBr', alpha=0.9, origin='lower', aspect='auto')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title(\"CMB with Edge Overlay (cleaned)\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cmb_edge_overlay_cleaned.png\", dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "4b455d9a45c7e271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Nonlinear exaggeration\n",
    "edges_exaggerated = edges**0.9  # or try 2.0\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(edges_exaggerated, cmap='turbo')\n",
    "plt.title('Possible Cosmic String Candidates (Edges in CMB)')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "id": "2b9e6d04a881e700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sharp_edges = edges + 0.7 * edges  # amplify edge intensity\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(sharp_edges, cmap='inferno')\n",
    "plt.title('Possible Cosmic String Candidates (Edges in CMB)')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "9fea3233b6b0440d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Alter this code block\n",
    "# Apply edge detection (e.g., Sobel)edges = ndimage.sobel(img)\n",
    "# Plot the edges detected in the CMB map\n",
    "# Apply Sobel in x and y directions\n",
    "dx = ndimage.sobel(img, axis=0) ** 0.5\n",
    "dy = ndimage.sobel(img, axis=1) ** 0.5\n",
    "\n",
    "# Gradient magnitude\n",
    "edges = np.hypot(dx, dy)  # Same as sqrt(dx**2 + dy**2)\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.imshow(edges, cmap='inferno')\n",
    "plt.title('Possible Cosmic String Candidates (Edges in CMB)')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "ef325655c7cb6d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "frequencies = np.linspace(10e9, 1000e9, 1000)  # 10 GHz to 1000 GHz\n",
    "\n",
    "T_mean = 2.725\n",
    "delta_T = 100e-6  # 100 µK typical fluctuation\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean), label=\"T = 2.725 K\", color='blue')\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean + delta_T), '--', label=\"+100 µK\", color='green', alpha=0.8)\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean - delta_T), '--', label=\"-100 µK\", color='red', alpha=0.8)\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Spectral Radiance (W·sr⁻¹·m⁻²·Hz⁻¹)\")\n",
    "plt.title(\"CMB Spectrum with Fluctuation Bounds (±100 µK)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the CMB spectrum with +100 µK fluctuations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean), label=\"T = 2.725 K\", color='blue')\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean + delta_T), '--', label=\"+100 µK\", color='green', alpha=0.8)\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Spectral Radiance (W·sr⁻¹·m⁻²·Hz⁻¹)\")\n",
    "plt.title(\"CMB Spectrum with Fluctuation Bounds (±100 µK)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the CMB spectrum with -100 µK fluctuations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean), label=\"T = 2.725 K\", color='blue')\n",
    "plt.plot(frequencies / 1e9, planck(frequencies, T_mean - delta_T), '--', label=\"-100 µK\", color='red', alpha=0.8)\n",
    "plt.xlabel(\"Frequency (GHz)\")\n",
    "plt.ylabel(\"Spectral Radiance (W·sr⁻¹·m⁻²·Hz⁻¹)\")\n",
    "plt.title(\"CMB Spectrum with Fluctuation Bounds (±100 µK)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "15eadc28ec4474e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cl = hp.anafast(cmb_map)\n",
    "plt.plot(cl[:200])\n",
    "plt.title(\"Low-ℓ Multipoles – Texture Signature Region\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "de96fe4776a656fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# --- Compute power spectrum from CMB map ---\n",
    "cl = hp.anafast(cmb_map)         # Power spectrum\n",
    "ell = np.arange(len(cl))         # Multipole indices\n",
    "\n",
    "# --- Begin Plot ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot measured CMB power spectrum (excluding monopole)\n",
    "plt.loglog(ell[1:1500], cl[1:1500], label='CMB Power Spectrum', color='blue')\n",
    "\n",
    "# Plot theoretical scale-invariant reference curve\n",
    "ref_value = np.mean(cl[5:15])\n",
    "theory_curve = ref_value * (ell[5] * (ell[5] + 1)) / (ell[1:1500] * (ell[1:1500] + 1))\n",
    "plt.loglog(ell[1:1500], theory_curve, 'r--', label='Scale-invariant (∝ 1/ℓ(ℓ+1))')\n",
    "\n",
    "# --- Highlight Regions ---\n",
    "plt.axvspan(2, 30, color='gray', alpha=0.15, label='Sachs-Wolfe Region')\n",
    "plt.axvspan(50, 200, color='orange', alpha=0.08, label='Transition Region')\n",
    "plt.axvspan(200, 1500, color='green', alpha=0.08, label='Acoustic Peaks Region')\n",
    "\n",
    "# --- Annotations (repositioned to avoid overlap) ---\n",
    "plt.text(7, 2e-10, 'Sachs-Wolfe\\nSuper-horizon modes', fontsize=10, color='black')\n",
    "plt.text(60, 5e-20, 'Transition:\\nEarly Oscillations &\\n Projection Effects', fontsize=10, color='black')\n",
    "plt.text(300, 5e-12, 'Acoustic Peaks:\\nPhoton-Baryon Oscillations', fontsize=10, color='black')\n",
    "\n",
    "# 1st Peak marker\n",
    "plt.axvline(220, color='black', linestyle='--', alpha=0.5)\n",
    "plt.text(235, 1e-10, '1st Peak\\n(~ℓ=220)', fontsize=9, color='black')\n",
    "\n",
    "# --- Labels, Legend, Grid ---\n",
    "plt.title(\"CMB Angular Power Spectrum (Log Scale)\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.2)\n",
    "\n",
    "# --- Format axes using human-readable numbers ---\n",
    "def human_format(x, pos):\n",
    "    if x >= 1:\n",
    "        return f\"{int(x)}\"\n",
    "    else:\n",
    "        return f\"{x:.1g}\"\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(human_format))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(human_format))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "df030401fd974066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the power spectrum (ℓ = 0 to 199)\n",
    "ell = np.arange(len(cl))  # Define ell as the array of multipole indices\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(ell[:250], cl[:250], label='CMB Power')\n",
    "\n",
    "# Overlay topological defect regions\n",
    "plt.axvspan(2, 10, color='blue', alpha=0.3, label='Textures (ℓ ≈ 2–10)')\n",
    "plt.axvspan(2, 5, color='red', alpha=0.3, label='Domain Walls (ℓ ≈ 2–5)')\n",
    "plt.axvspan(50, 200, color='green', alpha=0.2, label='Cosmic Strings (ℓ ≈ 50–200)')\n",
    "plt.axvspan(2, 3, color='orange', alpha=0.4, label='Monopoles (ℓ ≈ 2–3)')\n",
    "\n",
    "# Labels and grid\n",
    "plt.title(\"CMB Multipole Spectrum with Topological Defect Regions\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "395f01dcb190477c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the power spectrum (ℓ = 0 to 199)\n",
    "ell = np.arange(len(cl))  # Define ell as the array of multipole indices\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(ell[:50], cl[:50], label='CMB Power')\n",
    "\n",
    "# Overlay topological defect regions\n",
    "plt.axvspan(2, 10, color='blue', alpha=0.3, label='Textures (ℓ ≈ 2–10)')\n",
    "plt.axvspan(2, 5, color='red', alpha=0.3, label='Domain Walls (ℓ ≈ 2–5)')\n",
    "plt.axvspan(2, 3, color='orange', alpha=0.4, label='Monopoles (ℓ ≈ 2–3)')\n",
    "\n",
    "# Labels and grid\n",
    "plt.title(\"CMB Multipole Spectrum with Topological Defect Regions\")\n",
    "plt.xlabel(\"Multipole ℓ [ℓ: inverse angular scale]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ea844d697ff10b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use same `cl` from previous block\n",
    "ell = np.arange(1, 200)  # Skip ℓ = 0 to avoid divide-by-zero\n",
    "theta_deg = 180 / ell    # Angular scale in degrees\n",
    "\n",
    "# Trim cl accordingly\n",
    "cl_trim = cl[1:200]\n",
    "\n",
    "# Plot C_ell vs angular scale θ\n",
    "plt.plot(theta_deg, cl_trim)\n",
    "plt.title(\"CMB Power Spectrum vs Angular Scale\")\n",
    "plt.xlabel(\"Angular Scale θ [degrees]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.gca().invert_xaxis()  # Larger angular scales on the left\n",
    "plt.show()"
   ],
   "id": "7638a875a151a36f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ℓ = 1 to 199 (skip ℓ = 0 to avoid division by zero)\n",
    "theta_deg = 180 / ell\n",
    "cl_trim = cl[1:200]\n",
    "\n",
    "# Plot power vs angular scale\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(theta_deg, cl_trim, label='CMB Power')\n",
    "\n",
    "# Overlay topological defect regions in degrees\n",
    "plt.axvspan(18, 90, color='blue', alpha=0.3, label='Textures (θ ≈ 18° to 90°)')\n",
    "plt.axvspan(36, 90, color='red', alpha=0.3, label='Domain Walls (θ ≈ 36° to 90°)')\n",
    "plt.axvspan(1, 4, color='green', alpha=0.2, label='Cosmic Strings (θ ≈ 1° to 4°)')\n",
    "plt.axvspan(60, 90, color='orange', alpha=0.4, label='Monopoles (θ ≈ 60° to 90°)')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(\"CMB Power vs Angular Scale with Topological Defect Regions\")\n",
    "plt.xlabel(\"Angular Scale θ [degrees]\")\n",
    "plt.ylabel(\"C_ℓ [Power: variance of temperature fluctuations]\")\n",
    "plt.grid()\n",
    "plt.gca().invert_xaxis()  # Large scales (low ℓ) on left\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2e2cb26eda2e7220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of exaggerated cosmic strings",
   "id": "1f62f590ae13c173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from skimage import draw  # Changed import to use skimage.draw instead of matplotlib.pyplot.draw\n",
    "\n",
    "def simulate_cosmic_strings(map_size, num_strings):\n",
    "    map_data = np.zeros((map_size, map_size))\n",
    "    for _ in range(num_strings):\n",
    "        x = np.random.randint(0, map_size)\n",
    "        y = np.random.randint(0, map_size)\n",
    "        dx = np.random.randint(-map_size//4, map_size//4)\n",
    "        dy = np.random.randint(-map_size//4, map_size//4)\n",
    "        rr, cc = draw.line(x, y, x+dx, y+dy)  # Now using skimage.draw.line\n",
    "        map_data[rr % map_size, cc % map_size] += np.random.choice([-1, 1]) * 100e-6  # ~100 µK jump\n",
    "    return map_data\n",
    "\n",
    "# Simulate and plot\n",
    "map_size = 1000  # Reduced size for better visualization\n",
    "num_strings = 10\n",
    "cosmic_map = simulate_cosmic_strings(map_size, num_strings)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cosmic_map, cmap='RdBu_r')\n",
    "plt.colorbar(label='Temperature fluctuation (K)')\n",
    "plt.title('Simulated Cosmic Strings')\n",
    "plt.show()\n"
   ],
   "id": "127d1aff7138c278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploring the data with Machine Learning Methods\n",
    "\n",
    "In this section, the application of various machine learning techniques to analyze the CMB data and identify potential patterns or anomalies that might be associated with cosmic strings or other phenomena.\n",
    "\n",
    "Ensure the data and necessary libraries are loaded (just means sections below are not reliant on sections above.)\n"
   ],
   "id": "3918d60f92e61230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary ML libraries\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Make sure we have the CMB map loaded\n",
    "# If not already loaded, load it again\n",
    "if 'cmb_map' not in locals():\n",
    "    cmb_map = hp.read_map('data/COM_CMB_IQU-smica_2048_R3.00_full.fits', field=0)\n",
    "    nside = hp.get_nside(cmb_map)\n",
    "    npix = hp.nside2npix(nside)\n",
    "    img = hp.mollview(cmb_map, return_projected_map=True, nest=False, title='CMB Temperature Map', cmap='inferno', xsize=2000, hold=True)\n",
    "    img_cleaned = np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"CMB map shape: {cmb_map.shape}\")\n",
    "print(f\"Projected image shape: {img_cleaned.shape}\")\n"
   ],
   "id": "94951ca05a9d602b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Feature Extraction and Dimensionality Reduction\n",
    "\n",
    "First, extracting features from the CMB data and reducing dimensionality to visualize patterns.\n"
   ],
   "id": "4816520929fc27d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Single Cluster",
   "id": "b6ecb64e64110d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For ML analysis, we'll work with the 2D projected map (img_cleaned)\n",
    "# Let's extract patches from the image to use as features\n",
    "\n",
    "def extract_patches(image, patch_size=16, stride=8):\n",
    "    \"\"\"Extract patches from a 2D image with a given stride.\"\"\"\n",
    "    patches = []\n",
    "    positions = []\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            # Skip patches with NaN or inf values\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            if np.isfinite(patch).all() and not np.isnan(patch).any():\n",
    "                # Flatten the patch to a 1D array\n",
    "                patches.append(patch.flatten())\n",
    "                positions.append((i, j))\n",
    "\n",
    "    return np.array(patches), positions\n",
    "\n",
    "# Extract patches from the cleaned image\n",
    "# patch_size = 4 , stride = 2\tVery localized features, fast\tNo texture, easy to overfit noise\n",
    "# patch_size = 8 , stride = 4\tUltra-local\tVery sensitive to noise; good for edge detection\n",
    "# patch_size = 16, stride = 8\tSmall patches, overlapping\tDense coverage, good for texture analysis\n",
    "# patch_size = 32, stride = 32\tLarge, non-overlapping\tCoarse, fewer samples but high information per patch\n",
    "# patch_size = 64, stride = 16\tLarge and overlapping\tHeavy computation; useful for large-scale pattern mining\n",
    "#\n",
    "#\n",
    "# Try three experimental combinations:\n",
    "# Test\tPatch Size\tStride\tUse Case\n",
    "# A\t    16\t        8\t    Balanced: local structure + manageable data\n",
    "# B\t    32\t        16\t    Large structures, good for global coherence\n",
    "# C\t    8\t        4\t    Detect sharp features (e.g., cosmic strings, shocks)\n",
    "patch_size = 8\n",
    "stride = 4\n",
    "patches, positions = extract_patches(img_cleaned, patch_size, stride)\n",
    "\n",
    "print(f\"Extracted {len(patches)} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "# Standardize the patches\n",
    "scaler = StandardScaler()\n",
    "patches_scaled = scaler.fit_transform(patches)\n",
    "\n",
    "# Fit PCA with all components first to inspect explained variance\n",
    "pca_full = PCA()\n",
    "pca_full.fit(patches_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Define your target variance threshold (e.g., 95%)\n",
    "#     target_variance: A pre-defined threshold (e.g., 0.95 for 95%) indicating how much of the\n",
    "#     original data's variability PCA should preserve.\n",
    "#\n",
    "#     Purpose: Balances dimensionality reduction against information loss.\n",
    "target_variance = 0.95\n",
    "\n",
    "# Find the number of components that meet or exceed this threshold\n",
    "n_components = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "print(f\"Number of components to retain {target_variance*100:.0f}% variance: {n_components}\")\n",
    "\n",
    "\n",
    "# Apply PCA again with the optimal number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "patches_pca = pca.fit_transform(patches_scaled)\n",
    "\n",
    "# Reconstruct data with selected components\n",
    "patches_reduced = pca.fit_transform(patches_scaled)\n",
    "patches_reconstructed = pca.inverse_transform(patches_reduced)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = np.mean((patches_scaled - patches_reconstructed) ** 2)\n",
    "print(f\"Reconstruction MSE: {mse:.2e}\")\n",
    "\n",
    "# Optional: plot the explained variance curve with a line at the threshold\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_variance, label='Cumulative Explained Variance')\n",
    "plt.axhline(y=target_variance, color='r', linestyle='--', label=f'{target_variance*100:.0f}% Variance')\n",
    "plt.axvline(x=n_components - 1, color='g', linestyle='--', label=f'{n_components} Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Apply t-SNE for visualization\n",
    "# t-Distributed Stochastic Neighbor Embedding\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "patches_tsne = tsne.fit_transform(patches_pca)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(patches_tsne[:, 0], patches_tsne[:, 1], alpha=0.5, s=5)\n",
    "plt.title('t-SNE Visualization of CMB Patches')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "5645af766ac7a64e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Is This MSE Value Good?\n",
    "\n",
    "Typical MSE Range for CMB Data:\n",
    "\n",
    "Excellent Reconstruction: MSE < 1e-02 (near-perfect preservation of CMB features)\n",
    "\n",
    "Good Reconstruction: MSE ~1e-02 to 1e-01 (retains most cosmological signals)\n",
    "\n",
    "Moderate Reconstruction: MSE ~1e-01 to 5e-01 (some loss of small-scale fluctuations)\n",
    "\n",
    "Poor Reconstruction: MSE > 5e-01 (significant smoothing/feature loss)\n",
    "\n",
    "Aim for MSE < 1e-02 to ensure topology-sensitive features survive dimensionality reduction."
   ],
   "id": "9ea2892a132ffd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if mse < 1e-02:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is excellent for CMB data\")\n",
    "elif mse < 1e-01:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is good for CMB data\")\n",
    "elif mse < 5e-01:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is moderate for CMB data\")\n",
    "else:\n",
    "    print(f\"Reconstruction MSE: {mse:.2e} is not good for CMB data\")"
   ],
   "id": "25711d4025912981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Combined Clusters",
   "id": "8e95731d5bc79428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# --- PATCH EXTRACTION FUNCTION ---\n",
    "def extract_patches(image, patch_size=16, stride=8):\n",
    "    \"\"\"Extract patches from a 2D image with a given stride.\"\"\"\n",
    "    patches = []\n",
    "    positions = []\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            if np.isfinite(patch).all() and not np.isnan(patch).any():\n",
    "                patches.append(patch.flatten())\n",
    "                positions.append((i, j))\n",
    "\n",
    "    return np.array(patches), positions\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "patch_sizes = [8, 16, 32, 64]  # Multi-scale patch sizes\n",
    "stride = 4\n",
    "target_variance = 0.95\n",
    "\n",
    "# --- COLLECT ALL FEATURES ---\n",
    "all_features = []\n",
    "n_components_dict = {}\n",
    "explained_variances = {}\n",
    "mse_dict = {}\n",
    "\n",
    "pca_models = {}\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "for patch_size in patch_sizes:\n",
    "    print(f\"\\n🔍 Processing patch size: {patch_size}x{patch_size}\")\n",
    "\n",
    "    # Extract patches\n",
    "    patches, _ = extract_patches(img_cleaned, patch_size, stride)\n",
    "    print(f\"Extracted {len(patches)} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    patches_scaled = scaler.fit_transform(patches)\n",
    "\n",
    "    # PCA to inspect variance\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(patches_scaled)\n",
    "    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "    # Determine components needed\n",
    "    n_components = np.argmax(cumulative_variance >= target_variance) + 1\n",
    "    n_components_dict[patch_size] = n_components\n",
    "    explained_variances[patch_size] = cumulative_variance\n",
    "\n",
    "    print(f\"✅ {n_components} components retain {target_variance*100:.0f}% variance\")\n",
    "\n",
    "    # Apply PCA with optimal components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    patches_pca = pca.fit_transform(patches_scaled)\n",
    "\n",
    "    # Reconstruct from PCA\n",
    "    patches_reconstructed = pca.inverse_transform(patches_pca)\n",
    "\n",
    "    # Compute reconstruction MSE\n",
    "    mse = np.mean((patches_scaled - patches_reconstructed) ** 2)\n",
    "    mse_dict[patch_size] = mse\n",
    "    print(f\"📉 Reconstruction MSE for {patch_size}x{patch_size} patches: {mse:.2e}\")\n",
    "\n",
    "    # Store PCA and scaler for later analysis\n",
    "    pca_models[patch_size] = pca\n",
    "    scalers[patch_size] = scaler\n",
    "\n",
    "    # Save features\n",
    "    all_features.append(patches_pca)\n",
    "\n",
    "\n",
    "# --- COMBINE MULTISCALE FEATURES ---\n",
    "# --- Find common patch count across all scales ---\n",
    "min_len = min(f.shape[0] for f in all_features)\n",
    "print(f\"\\n🔧 Truncating all features to {min_len} samples for alignment\")\n",
    "\n",
    "# Truncate all feature arrays to the minimum number of patches\n",
    "all_features_trimmed = [f[:min_len] for f in all_features]\n",
    "\n",
    "# Concatenate safely\n",
    "combined_features = np.concatenate(all_features_trimmed, axis=1)\n",
    "print(f\"🔗 Combined feature shape: {combined_features.shape}\")\n",
    "\n",
    "\n",
    "# --- OPTIONAL: Plot explained variance curves for each scale ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "for size in patch_sizes:\n",
    "    plt.plot(explained_variances[size], label=f'{size}x{size}')\n",
    "plt.axhline(y=target_variance, color='r', linestyle='--', label='Target Variance')\n",
    "plt.title('Cumulative Explained Variance per Patch Size')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- APPLY t-SNE ON COMBINED FEATURES ---\n",
    "print(\"\\n🎯 Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate='auto')\n",
    "features_tsne = tsne.fit_transform(combined_features)\n",
    "\n",
    "# --- PLOT T-SNE ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(features_tsne[:, 0], features_tsne[:, 1], alpha=0.5, s=5)\n",
    "plt.title('t-SNE of Multi-Scale PCA-Reduced CMB Patches')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- PRINT MSE SUMMARY ---\n",
    "print(\"\\n📊 MSE Summary by Patch Size:\")\n",
    "for size in patch_sizes:\n",
    "    print(f\"  - {size}x{size}: MSE = {mse_dict[size]:.2e}\")\n"
   ],
   "id": "755b29a505e4db26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Is This MSE Value Good?\n",
    "\n",
    "Typical MSE Range for CMB Data:\n",
    "\n",
    "Excellent Reconstruction: MSE < 1e-02 (near-perfect preservation of CMB features)\n",
    "\n",
    "Good Reconstruction: MSE ~1e-02 to 1e-01 (retains most cosmological signals)\n",
    "\n",
    "Moderate Reconstruction: MSE ~1e-01 to 5e-01 (some loss of small-scale fluctuations)\n",
    "\n",
    "Poor Reconstruction: MSE > 5e-01 (significant smoothing/feature loss)\n",
    "\n",
    "Aim for MSE < 1e-02 to ensure topology-sensitive features survive dimensionality reduction."
   ],
   "id": "9cb762c98ba5537a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Evaluate Reconstruction Quality ---\n",
    "print(\"📌 MSE Quality Assessment per Patch Size:\")\n",
    "for size in patch_sizes:\n",
    "    mse = mse_dict[size]\n",
    "    if mse < 1e-02:\n",
    "        print(f\"  ✅ {size}x{size}: Reconstruction MSE = {mse:.2e} is **excellent** for CMB data\")\n",
    "    elif mse < 1e-01:\n",
    "        print(f\"  👍 {size}x{size}: Reconstruction MSE = {mse:.2e} is **good** for CMB data\")\n",
    "    elif mse < 5e-01:\n",
    "        print(f\"  ⚠️ {size}x{size}: Reconstruction MSE = {mse:.2e} is **moderate** for CMB data\")\n",
    "    else:\n",
    "        print(f\"  ❌ {size}x{size}: Reconstruction MSE = {mse:.2e} is **not good** for CMB data\")\n"
   ],
   "id": "60d17748bbdee0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Unsupervised Learning: Clustering Analysis\n",
    "\n",
    "Now we'll apply clustering algorithms to identify groups of similar patterns in the CMB data.\n"
   ],
   "id": "732ef428e849ab2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Dynamic Optimization of Clustering Parameters\n",
    "\n",
    "To find the optimal clustering of the CMB data, a dynamic approach is used that:\n",
    "\n",
    "1. Tries different numbers of clusters (from 3 to 50)\n",
    "2. For each number of clusters, runs K-means multiple times with different random initializations\n",
    "3. Calculates the silhouette score for each clustering attempt\n",
    "4. Selects the clustering with the highest silhouette score\n",
    "\n",
    "This approach helps find the best clustering configuration automatically, rather than manually tuning parameters. The silhouette score measures how well-separated the clusters are, with higher scores indicating better-defined clusters.\n"
   ],
   "id": "8c446d0e0e97248e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Expected Silhouette Score Range for CMB Data\n",
    "\n",
    "    Good clustering:\n",
    "\n",
    "    0.5 - 1.0 → Strong evidence of cluster structure (rare for CMB unless studying clear anomalies like cold spots or non-Gaussian features).\n",
    "\n",
    "    0.3 - 0.5 → Reasonable separation (may indicate subtle non-Gaussianities or foreground contamination).\n",
    "\n",
    "    Ambiguous clustering:\n",
    "\n",
    "    0.1 - 0.3 → Weak structure (common for Gaussian CMB fluctuations; clusters may be artificial).\n",
    "\n",
    "    No meaningful clusters:\n",
    "\n",
    "    ≤ 0.1 or negative → Likely noise or overfitting (common if forcing clusters on Gaussian random fields).\n",
    "\n",
    "Key Insight:\n",
    "CMB is mostly Gaussian, so high silhouette scores are unexpected unless you're targeting specific anomalies or foregrounds. A score of 0.2-0.4 might be the realistic upper limit for most analyses.\n"
   ],
   "id": "7062a756daaf774b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Single Cluster",
   "id": "810be95ce2be9d70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to run K-means clustering multiple times and find the best silhouette score\n",
    "def find_best_kmeans(data, min_clusters=2, max_clusters=10, n_attempts=10):\n",
    "    \"\"\"\n",
    "    Run K-means clustering multiple times with different parameters to find the best silhouette score.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The data to cluster\n",
    "    min_clusters : int\n",
    "        Minimum number of clusters to try\n",
    "    max_clusters : int\n",
    "        Maximum number of clusters to try\n",
    "    n_attempts : int\n",
    "        Number of random initializations to try for each number of clusters\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_kmeans : KMeans\n",
    "        The best KMeans model\n",
    "    best_labels : array\n",
    "        The cluster labels from the best model\n",
    "    best_n_clusters : int\n",
    "        The number of clusters in the best model\n",
    "    best_score : float\n",
    "        The silhouette score of the best model\n",
    "    \"\"\"\n",
    "    best_score = -1\n",
    "    current_best_kmeans = -100\n",
    "    best_kmeans = None\n",
    "    best_labels = None\n",
    "    best_n_clusters = 0\n",
    "    break_score = 0\n",
    "\n",
    "    # Try different numbers of clusters\n",
    "    for n_clusters in range(min_clusters, max_clusters + 1):\n",
    "        print(f\"Trying {n_clusters} clusters...\")\n",
    "        if break_score > 3:\n",
    "            break\n",
    "\n",
    "        # Try multiple random initializations for each number of clusters\n",
    "        for attempt in range(n_attempts):\n",
    "            # Initialize and fit KMeans\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=attempt)\n",
    "            labels = kmeans.fit_predict(data)\n",
    "            if break_score  > 3:\n",
    "                break\n",
    "\n",
    "            # Calculate silhouette score\n",
    "            score = silhouette_score(data, labels)\n",
    "            if score < current_best_kmeans:\n",
    "                print(f\"scored lower ({score}) than previous best, breaking out of loop\")\n",
    "                break_score = break_score + 1\n",
    "                break\n",
    "            current_best_kmeans = score\n",
    "            print(f\"  Attempt {attempt+1}/{n_attempts}: Silhouette Score = {score:.9f}\")\n",
    "\n",
    "            # Update best model if this one is better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_kmeans = kmeans\n",
    "                best_labels = labels\n",
    "                best_n_clusters = n_clusters\n",
    "                print(f\"  New best score: {best_score:.9f} with {best_n_clusters} clusters\")\n",
    "            if score <= 0.1:\n",
    "                print(f\"  No meaningful clusters found for {n_clusters} clusters\")\n",
    "                break\n",
    "            if score <= best_score-0.05:\n",
    "                print(f\"  No improvement in silhouette score for {n_clusters} clusters\")\n",
    "                break\n",
    "            break_score = 0\n",
    "\n",
    "    print(f\"\\nBest clustering: {best_n_clusters} clusters with silhouette score {best_score:.9f}\")\n",
    "    return best_kmeans, best_labels, best_n_clusters, best_score\n",
    "\n",
    "# Apply K-means clustering with multiple attempts to find the best silhouette score\n",
    "min_clusters = 3\n",
    "max_clusters = 10\n",
    "n_attempts = 5\n",
    "print(f\"Finding best K-means clustering (trying {min_clusters}-{max_clusters} clusters, {n_attempts} attempts each)...\")\n",
    "best_kmeans, cluster_labels, best_n_clusters, best_silhouette = find_best_kmeans(\n",
    "    patches_pca, min_clusters=min_clusters, max_clusters=max_clusters, n_attempts=n_attempts\n",
    ")\n",
    "\n",
    "# Visualize clusters in t-SNE space\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(patches_tsne[:, 0], patches_tsne[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'K-means Clustering (k={best_n_clusters}, Silhouette={best_silhouette:.3f})')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Calculate silhouette score to evaluate clustering quality\n",
    "#     Higher Silhouette Score → Better clustering quality \n",
    "#     (points are correctly assigned to tight, well-separated clusters).\n",
    "# \n",
    "#     Lower Silhouette Score → Worse clustering quality \n",
    "#     (points may be misassigned or clusters overlap).\n",
    "# 0.2-0.4 would be a good score for CMB data due to the nature of it\n",
    "print(f\"Best Silhouette Score: {best_silhouette:.3f} with {best_n_clusters} clusters\")\n",
    "\n",
    "# Visualize cluster centers in image space\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(best_n_clusters):\n",
    "    plt.subplot(1, best_n_clusters, i+1)\n",
    "    # Get the center of the cluster in original space\n",
    "    center = pca.inverse_transform(best_kmeans.cluster_centers_[i])\n",
    "    center = scaler.inverse_transform([center])[0]\n",
    "    # Reshape to patch size\n",
    "    center = center.reshape(patch_size, patch_size)\n",
    "    plt.imshow(center, cmap='inferno')\n",
    "    plt.title(f'Cluster {i}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Map clusters back to the original image\n",
    "cluster_map = np.zeros(img_cleaned.shape)\n",
    "cluster_count = np.zeros(img_cleaned.shape)\n",
    "\n",
    "for (i, j), label in zip(positions, cluster_labels):\n",
    "    cluster_map[i:i+patch_size, j:j+patch_size] += label\n",
    "    cluster_count[i:i+patch_size, j:j+patch_size] += 1\n",
    "\n",
    "# Average the cluster labels where patches overlap\n",
    "mask = cluster_count > 0\n",
    "cluster_map[mask] /= cluster_count[mask]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cluster_map, cmap='viridis')\n",
    "plt.title(f'Cluster Map of CMB Data (k={best_n_clusters}, Silhouette={best_silhouette:.3f})')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ],
   "id": "c32d1c7c10cce2cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check variance retention\n",
    "# Aim for ≥95% for CMB (unlike images, where 80-90% may suffice).\n",
    "print(f\"Variance retained: {np.sum(pca.explained_variance_ratio_):.2%}\")"
   ],
   "id": "f40a1aa650fb1318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## For Combined Clusters",
   "id": "7727615cdbefb6e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- PREP: Store clustering results ---\n",
    "best_kmeans_models = {}\n",
    "cluster_labels_dict = {}\n",
    "\n",
    "# Choose patch sizes for fusion\n",
    "selected_patch_sizes = patch_sizes  # example: use 4x4 and 16x16\n",
    "\n",
    "# Apply silhouette-optimized KMeans per selected patch size\n",
    "for size in selected_patch_sizes:\n",
    "    print(f\"\\n🔍 Running KMeans with silhouette scoring on {size}x{size} patches\")\n",
    "\n",
    "    features = all_features[patch_sizes.index(size)][:min_len]  # truncate to match others\n",
    "    best_kmeans, labels, n_clusters, silhouette = find_best_kmeans(\n",
    "        features, min_clusters=3, max_clusters=20, n_attempts=10\n",
    "    )\n",
    "\n",
    "    best_kmeans_models[size] = best_kmeans\n",
    "    cluster_labels_dict[size] = labels\n",
    "    print(f\"✅ Best k={n_clusters} with silhouette score={silhouette:.3f}\")\n",
    "\n",
    "# --- COMBINE MULTISCALE LABELS INTO META-FEATURE VECTOR ---\n",
    "print(\"\\n🔗 Building meta-feature vector from cluster labels\")\n",
    "meta_features = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "print(f\"Meta-feature shape: {meta_features.shape}\")\n",
    "\n",
    "# --- APPLY ANOMALY DETECTION TO META-FEATURE VECTOR ---\n",
    "print(\"\\n🌌 Running Isolation Forest on cluster fusion features\")\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso.fit_predict(meta_features)\n",
    "\n",
    "# Convert to anomaly score (1 = normal, -1 = anomaly → flip)\n",
    "anomaly_scores = -1 * anomaly_labels\n",
    "print(f\"Anomaly scores: {np.unique(anomaly_scores, return_counts=True)}\")\n"
   ],
   "id": "61c0f80237e0eb19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- VISUALIZE IN T-SNE SPACE ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(features_tsne[:len(anomaly_scores), 0], features_tsne[:len(anomaly_scores), 1],\n",
    "            c=anomaly_scores, cmap='coolwarm', alpha=0.7, s=10)\n",
    "plt.title('Anomaly Detection from Multi-Scale Cluster Fusion')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5d893c517182604c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply K-means clustering with multiple attempts to find the best silhouette score\n",
    "min_clusters = 3\n",
    "max_clusters = 150\n",
    "n_attempts = 50\n",
    "\n",
    "print(f\"\\n🔎 Finding best K-means clustering on multi-scale features ({min_clusters}-{max_clusters} clusters, {n_attempts} attempts each)...\")\n",
    "\n",
    "# Use the multi-scale features for clustering\n",
    "best_kmeans, cluster_labels, best_n_clusters, best_silhouette = find_best_kmeans(\n",
    "    combined_features, min_clusters=min_clusters, max_clusters=max_clusters, n_attempts=n_attempts\n",
    ")\n",
    "\n",
    "# Visualize clusters in t-SNE space (already computed from combined_features)\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'K-means Clustering on Multi-Scale Features (k={best_n_clusters}, Silhouette={best_silhouette:.3f})')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Report silhouette quality (typical good range is 0.2–0.4 for noisy CMB)\n",
    "print(f\"📈 Best Silhouette Score: {best_silhouette:.3f} with {best_n_clusters} clusters\")\n"
   ],
   "id": "6c4da5eaff0d2269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Alternate Method",
   "id": "be434ef5e46d7817"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- PREP: Store clustering results ---\n",
    "best_kmeans_models = {}\n",
    "cluster_labels_dict = {}\n",
    "silhouette_scores_per_size = {}\n",
    "entropy_scores = {}\n",
    "\n",
    "# --- CHOOSE PATCH SIZES FOR META-FUSION ---\n",
    "selected_patch_sizes = patch_sizes  # or e.g., [4, 8, 16]\n",
    "\n",
    "# --- SILHOUETTE-OPTIMIZED KMEANS CLUSTERING ---\n",
    "for size in selected_patch_sizes:\n",
    "    print(f\"\\n🔍 Running KMeans on {size}x{size} features with silhouette scoring\")\n",
    "\n",
    "    features = all_features[patch_sizes.index(size)][:min_len]  # align patch count\n",
    "\n",
    "    sil_scores = []\n",
    "    all_models = []\n",
    "    all_labels = []\n",
    "\n",
    "    for k in range(3, 21):\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = km.fit_predict(features)\n",
    "        sil = silhouette_score(features, labels)\n",
    "        sil_scores.append(sil)\n",
    "        all_models.append(km)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    best_idx = int(np.argmax(sil_scores))\n",
    "    best_k = best_idx + 3\n",
    "    best_kmeans = all_models[best_idx]\n",
    "    best_labels = all_labels[best_idx]\n",
    "    best_score = sil_scores[best_idx]\n",
    "\n",
    "    best_kmeans_models[size] = best_kmeans\n",
    "    cluster_labels_dict[size] = best_labels\n",
    "    silhouette_scores_per_size[size] = best_score\n",
    "\n",
    "    # Compute entropy of cluster label distribution\n",
    "    label_counts = np.bincount(best_labels)\n",
    "    probs = label_counts / np.sum(label_counts)\n",
    "    ent = entropy(probs)\n",
    "    entropy_scores[size] = ent\n",
    "\n",
    "    print(f\"✅ Best k={best_k} | Silhouette={best_score:.3f} | Entropy={ent:.3f}\")\n",
    "\n",
    "    # Plot silhouette curve\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(range(3, 21), sil_scores, marker='o')\n",
    "    plt.title(f'Silhouette Scores for {size}x{size}')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- META-FEATURE COMBINATION ---\n",
    "print(\"\\n🔗 Combining cluster labels into meta-feature vectors\")\n",
    "meta_features = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "print(f\"Meta-feature shape: {meta_features.shape}\")\n",
    "\n",
    "# --- META-FEATURE DIAGNOSTICS ---\n",
    "\n",
    "# Correlation Matrix\n",
    "print(\"\\n📊 Label Correlation Matrix (Pearson):\")\n",
    "corr_matrix = np.corrcoef(meta_features.T)\n",
    "print(np.round(corr_matrix, 3))\n",
    "\n",
    "# Entropy of each label column\n",
    "print(\"\\n🧠 Entropy of Cluster Label Distributions:\")\n",
    "for size in selected_patch_sizes:\n",
    "    print(f\"  - {size}x{size}: Entropy = {entropy_scores[size]:.3f}\")\n",
    "\n",
    "# --- ISOLATION FOREST FOR ANOMALY DETECTION ---\n",
    "print(\"\\n🌌 Running Isolation Forest on meta-cluster features...\")\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso.fit_predict(meta_features)\n",
    "\n",
    "# Convert -1 to 1 (anomaly), 1 to 0 (normal)\n",
    "anomaly_scores = -1 * (anomaly_labels - 1) // 2\n",
    "\n",
    "# Summary\n",
    "unique_vals, counts = np.unique(anomaly_scores, return_counts=True)\n",
    "print(f\"🧭 Anomaly score distribution: {dict(zip(unique_vals, counts))} (1 = Anomaly)\")\n",
    "\n",
    "# --- T-SNE FOR ANOMALY VISUALIZATION ---\n",
    "print(\"\\n🧪 Visualizing anomalies in t-SNE space...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "meta_tsne = tsne.fit_transform(meta_features)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(meta_tsne[:, 0], meta_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', s=10, alpha=0.7)\n",
    "plt.title('t-SNE of Meta-Cluster Features (Anomaly Detection)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Anomaly Score (1 = Anomaly)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "7323938f9f1b888d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- CHECK: Silhouette scores for each selected patch size ---\n",
    "print(\"\\n📈 Silhouette Scores per Patch Size:\")\n",
    "for size in selected_patch_sizes:\n",
    "    sil = silhouette_scores_per_size.get(size, None)\n",
    "    if sil is not None:\n",
    "        print(f\"  - {size}x{size}: Silhouette = {sil:.3f}\")\n",
    "    else:\n",
    "        print(f\"  - {size}x{size}: ❌ Not found\")\n",
    "\n",
    "# --- CHECK: Entropy of cluster label distributions ---\n",
    "print(\"\\n🧠 Entropy of Cluster Label Distributions:\")\n",
    "for size in selected_patch_sizes:\n",
    "    labels = cluster_labels_dict[size]\n",
    "    counts = np.bincount(labels)\n",
    "    probs = counts / np.sum(counts)\n",
    "    ent = entropy(probs)\n",
    "    print(f\"  - {size}x{size}: Entropy = {ent:.3f}\")\n",
    "\n",
    "# --- CHECK: Label correlation matrix ---\n",
    "print(\"\\n🔗 Correlation Matrix Between Cluster Label Sets:\")\n",
    "\n",
    "# Create matrix from all selected label vectors\n",
    "label_matrix = np.column_stack([cluster_labels_dict[size] for size in selected_patch_sizes])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = np.corrcoef(label_matrix.T)\n",
    "\n",
    "# Display as matrix\n",
    "print(\"    Patch Sizes:\", selected_patch_sizes)\n",
    "print(np.round(corr_matrix, 3))\n",
    "\n",
    "# Optional: visualize the correlation matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.xticks(range(len(selected_patch_sizes)), [f'{s}x{s}' for s in selected_patch_sizes])\n",
    "plt.yticks(range(len(selected_patch_sizes)), [f'{s}x{s}' for s in selected_patch_sizes])\n",
    "plt.title(\"Cluster Label Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "bcd54c9869c9231f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Misc.",
   "id": "861245fb285d48d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare to UMAP (often more stable)\n",
    "from umap import UMAP\n",
    "umap_emb = UMAP(n_components=2).fit_transform(patches_pca)"
   ],
   "id": "dbd8e2f7382c527",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Overlay clusters on a CMB map\n",
    "plt.imshow(cluster_map, cmap='viridis', alpha=0.5)\n",
    "plt.imshow(cmb_map, cmap='inferno', alpha=0.5)"
   ],
   "id": "99144d40a0595806",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "labels = clusterer.fit_predict(patches_pca)"
   ],
   "id": "f22305733ef60d95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inertias = []\n",
    "for k in range(min_clusters, max_clusters+1):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(patches_pca)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(min_clusters, max_clusters+1), inertias, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ],
   "id": "f0b2e6f2a8bcbe41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check cluster sizes\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Visualize problematic clusters\n",
    "problem_cluster = 0  # Example\n",
    "problem_indices = np.where(cluster_labels == problem_cluster)[0]\n",
    "plt.scatter(patches_tsne[problem_indices, 0],\n",
    "           patches_tsne[problem_indices, 1])\n",
    "plt.title(f'Problematic Cluster {problem_cluster}')\n",
    "plt.show()"
   ],
   "id": "c413db825255872a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Anomaly Detection\n",
    "\n",
    "Using Isolation Forest to detect anomalies in the CMB data that might correspond to cosmic strings or other interesting features.\n"
   ],
   "id": "5b4933f658f780f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Singular Clusters",
   "id": "5092f045520fa9e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply Isolation Forest for anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_scores = iso_forest.fit_predict(patches_pca)\n",
    "\n",
    "# Convert to anomaly score (higher = more anomalous)\n",
    "anomaly_scores = -1 * anomaly_scores  # -1 becomes +1 (anomaly), 1 becomes -1 (normal)\n",
    "print(f\"Anomaly Score Range: {anomaly_scores.min()} to {anomaly_scores.max()}\")\n",
    "\n",
    "# Visualize anomalies in t-SNE space\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(patches_tsne[:, 0], patches_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Anomaly Score')\n",
    "plt.title('Anomaly Detection in CMB Patches')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Map anomaly scores back to the original image\n",
    "anomaly_map = np.zeros(img_cleaned.shape)\n",
    "anomaly_count = np.zeros(img_cleaned.shape)\n",
    "\n",
    "for (i, j), score in zip(positions, anomaly_scores):\n",
    "    anomaly_map[i:i+patch_size, j:j+patch_size] += score\n",
    "    anomaly_count[i:i+patch_size, j:j+patch_size] += 1\n",
    "\n",
    "# Average the anomaly scores where patches overlap\n",
    "mask = anomaly_count > 0\n",
    "anomaly_map[mask] /= anomaly_count[mask]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(anomaly_map, cmap='coolwarm')\n",
    "plt.title('Anomaly Map of CMB Data (Potential Cosmic String Candidates)')\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.show()\n"
   ],
   "id": "6cb05b2ec54aa324",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use 8x8 patch size for mapping (or adjust to your preference)\n",
    "chosen_patch_size = 8\n",
    "_, positions = extract_patches(img_cleaned, patch_size=chosen_patch_size, stride=stride)\n",
    "\n",
    "# Make sure anomaly scores are the same length as positions\n",
    "min_len = min(len(positions), len(anomaly_scores))\n",
    "positions = positions[:min_len]\n",
    "anomaly_scores = anomaly_scores[:min_len]\n",
    "\n",
    "# --- Build anomaly map ---\n",
    "anomaly_map = np.zeros_like(img_cleaned)\n",
    "anomaly_count = np.zeros_like(img_cleaned)\n",
    "\n",
    "for (i, j), score in zip(positions, anomaly_scores):\n",
    "    anomaly_map[i:i+chosen_patch_size, j:j+chosen_patch_size] += score\n",
    "    anomaly_count[i:i+chosen_patch_size, j:j+chosen_patch_size] += 1\n",
    "\n",
    "# Average overlapping patches\n",
    "mask = anomaly_count > 0\n",
    "anomaly_map[mask] /= anomaly_count[mask]\n",
    "\n",
    "# Optional: clip for visualization clarity\n",
    "vmin, vmax = np.percentile(anomaly_map[mask], [5, 95])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(anomaly_map, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.title('CMB Anomaly Map (Potential Topological Signatures)')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "3b2f798bbd402144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show top N anomalies\n",
    "N = 10\n",
    "top_indices = np.argsort(anomaly_scores)[-N:]\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for idx, i in enumerate(top_indices):\n",
    "    patch = scaler.inverse_transform(pca.inverse_transform(combined_features[i]))[:chosen_patch_size**2]\n",
    "    patch_img = patch.reshape(chosen_patch_size, chosen_patch_size)\n",
    "    plt.subplot(1, N, idx + 1)\n",
    "    plt.imshow(patch_img, cmap='inferno')\n",
    "    plt.title(f'Anomaly {idx+1}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Top Anomalous Patches (Multi-scale features)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "c6c6bf9c0ea6ac3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Multiple Clusters",
   "id": "11ec13c887c3f9a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# --- Apply Isolation Forest on combined multi-scale features ---\n",
    "print(\"\\n🌌 Running Isolation Forest for anomaly detection...\")\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(combined_features)\n",
    "anomaly_scores = iso_forest.decision_function(combined_features)  # Higher = more normal\n",
    "\n",
    "# Invert for easier interpretation (higher = more anomalous)\n",
    "anomaly_scores = -anomaly_scores\n",
    "print(f\"Anomaly score range: {anomaly_scores.min():.4f} to {anomaly_scores.max():.4f}\")\n",
    "\n",
    "# --- Visualize in t-SNE space ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=anomaly_scores, cmap='coolwarm', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Anomaly Score')\n",
    "plt.title('t-SNE Projection with Anomaly Scores')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "3620069ead4198e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features (principal components) are most important for distinguishing clusters and anomalies.\n"
   ],
   "id": "697df93ca9035124"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Singular Clusters",
   "id": "ec24efdb92931941"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Analyze the most important principal components\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(n_components), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Importance of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analyze the first few principal components\n",
    "n_display = min(5, n_components)\n",
    "plt.figure(figsize=(15, 3*n_display))\n",
    "for i in range(n_display):\n",
    "    plt.subplot(n_display, 1, i+1)\n",
    "    component = pca.components_[i].reshape(1, -1)\n",
    "    component_image = scaler.inverse_transform(component)[0].reshape(patch_size, patch_size)\n",
    "    plt.imshow(component_image, cmap='coolwarm')\n",
    "    plt.title(f'Principal Component {i+1}')\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "80b1b9345b2ea876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Multiple Clusters",
   "id": "1c21f80abbe9690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Bar Plot: Explained Variance per Scale ---\n",
    "for size in patch_sizes:\n",
    "    pca = pca_models[size]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "    plt.title(f'Explained Variance by PCA Components — {size}x{size} patches')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "adafe0ebc7446216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Visualize Top N PCA Components ---\n",
    "n_display = 5\n",
    "\n",
    "for size in patch_sizes:\n",
    "    pca = pca_models[size]\n",
    "    scaler = scalers[size]\n",
    "    components = pca.components_\n",
    "    patch_len = size * size\n",
    "\n",
    "    print(f\"\\n🔬 Visualizing top {n_display} PCA components for {size}x{size} patches\")\n",
    "\n",
    "    plt.figure(figsize=(15, 3 * n_display))\n",
    "    for i in range(min(n_display, components.shape[0])):\n",
    "        # Get component and inverse-scale it\n",
    "        component = components[i].reshape(1, -1)\n",
    "        restored = scaler.inverse_transform(pca.inverse_transform(component))[0]\n",
    "        patch = restored[:patch_len].reshape(size, size)\n",
    "\n",
    "        plt.subplot(n_display, 1, i + 1)\n",
    "        plt.imshow(patch, cmap='coolwarm')\n",
    "        plt.colorbar()\n",
    "        plt.title(f'{size}x{size} Patch — Principal Component {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "4c0a54d25e1beb33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Correlation with Edge Detection\n",
    "\n",
    "Let's compare our ML-based anomaly detection with the edge detection performed earlier to see if they identify similar features.\n"
   ],
   "id": "fb828f9cb59926f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Singular Cluster",
   "id": "ddf06ce857e2ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure we have the edge detection results\n",
    "if 'edges' not in locals():\n",
    "    edges = ndimage.sobel(img_cleaned)\n",
    "\n",
    "# Normalize both maps for comparison\n",
    "edges_norm = (edges - np.min(edges)) / (np.max(edges) - np.min(edges))\n",
    "anomaly_norm = (anomaly_map - np.min(anomaly_map)) / (np.max(anomaly_map) - np.min(anomaly_map))\n",
    "\n",
    "# Calculate correlation between edge detection and anomaly detection\n",
    "valid_mask = ~np.isnan(edges_norm) & ~np.isnan(anomaly_norm)\n",
    "correlation = np.corrcoef(edges_norm[valid_mask].flatten(), anomaly_norm[valid_mask].flatten())[0, 1]\n",
    "print(f\"Correlation between edge detection and anomaly detection: {correlation:.3f}\")\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(edges_norm, cmap='inferno')\n",
    "plt.title('Edge Detection')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edges_norm, cmap='inferno', alpha=0.7)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm', alpha=0.3)\n",
    "plt.title('Edge + Anomaly Overlay')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "d7c18ae0c8994923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For Multiple Clusters",
   "id": "60c8b32b64a824a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "# --- ENSURE EDGE DETECTION EXISTS ---\n",
    "if 'edges' not in locals():\n",
    "    print(\"🔍 Computing Sobel edge detection...\")\n",
    "    edges = ndimage.sobel(img_cleaned)\n",
    "\n",
    "# --- NORMALIZE MAPS FOR COMPARISON ---\n",
    "def normalize_map(x):\n",
    "    x = np.nan_to_num(x)\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-8)\n",
    "\n",
    "edges_norm = normalize_map(edges)\n",
    "anomaly_norm = normalize_map(anomaly_map)\n",
    "\n",
    "# --- CREATE VALID MASK ---\n",
    "valid_mask = (~np.isnan(edges_norm)) & (~np.isnan(anomaly_norm))\n",
    "edges_flat = edges_norm[valid_mask].flatten()\n",
    "anomaly_flat = anomaly_norm[valid_mask].flatten()\n",
    "\n",
    "# --- CORRELATION CALCULATION ---\n",
    "if edges_flat.size > 0 and anomaly_flat.size > 0:\n",
    "    correlation = np.corrcoef(edges_flat, anomaly_flat)[0, 1]\n",
    "    print(f\"📈 Correlation between edge detection and anomaly detection: {correlation:.3f}\")\n",
    "else:\n",
    "    correlation = np.nan\n",
    "    print(\"⚠️ No valid pixels for correlation.\")\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(edges_norm, cmap='inferno')\n",
    "plt.title('🟠 Edge Detection (Sobel)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm')\n",
    "plt.title('🔵 Anomaly Score (IForest)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edges_norm, cmap='inferno', alpha=0.7)\n",
    "plt.imshow(anomaly_norm, cmap='coolwarm', alpha=0.3)\n",
    "plt.title('🎯 Edge + Anomaly Overlay')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a7f7958c6e6892e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "overlap_map = edges_norm * anomaly_norm  # or np.abs(edges_norm - anomaly_norm)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(overlap_map, cmap='plasma')\n",
    "plt.title('Overlap Map (Edge × Anomaly)')\n",
    "plt.colorbar(label='Overlap Intensity')\n",
    "plt.show()\n"
   ],
   "id": "4d0ab8a38a19ea0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "We've applied several machine learning techniques to analyze the CMB data:\n",
    "\n",
    "1. **Feature Extraction**: Extracted patches from the CMB map and reduced dimensionality with PCA\n",
    "2. **Clustering**: Identified distinct patterns in the CMB data using K-means\n",
    "3. **Anomaly Detection**: Used Isolation Forest to find unusual patterns that might correspond to cosmic strings\n",
    "4. **Feature Importance**: Analyzed which principal components are most significant\n",
    "5. **Correlation Analysis**: Compared ML-based anomaly detection with traditional edge detection\n",
    "\n",
    "These techniques provide complementary views of the CMB data and can help identify potential cosmic string candidates or other interesting features that might not be apparent through traditional analysis methods.\n",
    "\n",
    "The correlation between edge detection and anomaly detection suggests that ML methods can identify similar structures to traditional methods, but may also reveal additional patterns not captured by edge detection alone.\n"
   ],
   "id": "9c8752af6db19eb8"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
